id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/483:583,usability,command,command,583,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,usability,error,error,721,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:866,usability,input,input,866,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1014,usability,input,input,1014,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:118,availability,error,error,118,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:645,availability,Error,Error,645,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:146,deployability,fail,failed,146,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:672,deployability,fail,failed,672,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1027,deployability,fail,failed,1027,"S exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1382,deployability,fail,failed,1382,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1737,deployability,fail,failed,1737,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:539,interoperability,specif,specific,539,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:587,modifiability,interm,intermediate,587,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:118,performance,error,error,118,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:127,performance,parallel,parallel,127,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:645,performance,Error,Error,645,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:653,performance,parallel,parallel,653,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1008,performance,parallel,parallel,1008,"-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1363,performance,parallel,parallel,1363,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1718,performance,parallel,parallel,1718,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:146,reliability,fail,failed,146,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:672,reliability,fail,failed,672,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1027,reliability,fail,failed,1027,"S exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1382,reliability,fail,failed,1382,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1737,reliability,fail,failed,1737,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:8,safety,test,test,8,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:61,safety,test,test,61,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:118,safety,error,error,118,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:264,safety,input,input,264,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:449,safety,input,input,449,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:645,safety,Error,Error,645,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:790,safety,input,input,790,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1145,safety,input,input,1145,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1500,safety,input,input,1500,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1855,safety,input,input,1855,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:8,testability,test,test,8,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:61,testability,test,test,61,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:118,usability,error,error,118,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:264,usability,input,input,264,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:449,usability,input,input,449,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:645,usability,Error,Error,645,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:790,usability,input,input,790,"Another test-. I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1145,usability,input,input,1145,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1500,usability,input,input,1500,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1855,usability,input,input,1855,"(fasta bam and bed) and again I get this same error: . parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:97,availability,error,error,97,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:155,deployability,fail,failed,155,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:103,integrability,messag,message,103,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:103,interoperability,messag,message,103,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:97,performance,error,error,97,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:136,performance,parallel,parallel,136,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:208,performance,parallel,parallel,208,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:155,reliability,fail,failed,155,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:595,reliability,doe,does,595,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:97,safety,error,error,97,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:97,usability,error,error,97,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,availability,error,error,29,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:321,availability,error,error,321,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:382,availability,error,error,382,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:400,availability,error,error,400,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:57,deployability,Fail,Failed,57,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:466,deployability,fail,failed,466,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,performance,error,error,29,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:321,performance,error,error,321,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:373,performance,parallel,parallel,373,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:382,performance,error,error,382,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:400,performance,error,error,400,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:447,performance,parallel,parallel,447,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:57,reliability,Fail,Failed,57,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:466,reliability,fail,failed,466,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,safety,error,error,29,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:321,safety,error,error,321,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:382,safety,error,error,382,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:400,safety,error,error,400,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:584,safety,input,input,584,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:769,safety,input,input,769,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,usability,error,error,29,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:166,usability,statu,statusor,166,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:189,usability,statu,status,189,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:205,usability,statu,status,205,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:321,usability,error,error,321,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:382,usability,error,error,382,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:400,usability,error,error,400,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:584,usability,input,input,584,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:769,usability,input,input,769,"You are right I've found the error: . ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474. Fatal Python error: Aborted"" . It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:251,availability,down,downloaded,251,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:422,availability,down,download,422,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:351,deployability,Fail,Failed,351,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:459,deployability,fail,fails,459,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:289,energy efficiency,current,current,289,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:351,reliability,Fail,Failed,351,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:459,reliability,fail,fails,459,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3,usability,command,command,3,"my command:. samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474"". I get this:. >chr1:4655405-4655474. CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC. CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. . On the current fasta file it writes:. >chr1:4655405-4655474. [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:107,availability,error,error,107,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:165,deployability,fail,failed,165,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:113,integrability,messag,message,113,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:113,interoperability,messag,message,113,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:107,performance,error,error,107,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:146,performance,parallel,parallel,146,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:218,performance,parallel,parallel,218,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:165,reliability,fail,failed,165,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:617,reliability,doe,does,617,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:107,safety,error,error,107,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:866,safety,test,test,866,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:866,testability,test,test,866,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:107,usability,error,error,107,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:906,usability,person,personal,906,". > I'm happy to hear you have enjoyed my YouTube videos :). > . > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. > . > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). > . > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file? for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:30,availability,Down,Downloaded,30,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:91,availability,error,error,91,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:564,availability,Error,Error,564,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:599,availability,error,error,599,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1396,availability,Error,Error,1396,"ediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1976,availability,Restor,Restoring,1976,"3.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2083,availability,Restor,Restoring,2083,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2673,availability,Restor,Restoring,2673,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2780,availability,Restor,Restoring,2780,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3035,availability,error,error,3035,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:213,deployability,fail,failed,213,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1616,deployability,version,version,1616," again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 1398250532964",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1796,energy efficiency,estimat,estimator,1796,"ence"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2007,energy efficiency,model,models,2007,"bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2018,energy efficiency,model,model,2018,"gions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2114,energy efficiency,model,models,2114,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2125,energy efficiency,model,model,2125,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2504,energy efficiency,Optim,Optimization,2504,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2623,energy efficiency,model,modeling,2623,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2704,energy efficiency,model,models,2704,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2715,energy efficiency,model,model,2715,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2811,energy efficiency,model,models,2811,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2822,energy efficiency,model,model,2822,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:63,integrability,repositor,repository,63,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:605,integrability,messag,message,605,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1402,integrability,messag,message,1402,"results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1616,integrability,version,version,1616," again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 1398250532964",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2920,integrability,batch,batches,2920,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:63,interoperability,repositor,repository,63,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:605,interoperability,messag,message,605,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1402,interoperability,messag,message,1402,"results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1259,modifiability,interm,intermediate,1259,"alling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1480,modifiability,pac,packages,1480,"r/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:17",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1557,modifiability,layer,layer,1557," 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1616,modifiability,version,version,1616," again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 1398250532964",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1637,modifiability,layer,layer,1637,"ithout num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1685,modifiability,layer,layer,1685,"`. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring param",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1986,modifiability,paramet,parameters,1986,"wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2093,modifiability,paramet,parameters,2093,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2683,modifiability,paramet,parameters,2683,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2790,modifiability,paramet,parameters,2790,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:91,performance,error,error,91,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:194,performance,parallel,parallel,194,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:564,performance,Error,Error,564,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:599,performance,error,error,599,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1396,performance,Error,Error,1396,"ediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2504,performance,Optimiz,Optimization,2504,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2920,performance,batch,batches,2920,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3035,performance,error,error,3035,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:213,reliability,fail,failed,213,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1976,reliability,Restor,Restoring,1976,"3.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2083,reliability,Restor,Restoring,2083,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2673,reliability,Restor,Restoring,2673,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2780,reliability,Restor,Restoring,2780,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:91,safety,error,error,91,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:331,safety,input,input,331,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:516,safety,input,input,516,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:564,safety,Error,Error,564,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:599,safety,error,error,599,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,safety,input,input,721,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:730,safety,input,input,730,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:971,safety,input,input,971,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1028,safety,input,input,1028,"ownloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1396,safety,Error,Error,1396,"ediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3035,safety,error,error,3035,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2007,security,model,models,2007,"bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2018,security,model,model,2018,"gions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2114,security,model,models,2114,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2125,security,model,model,2125,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2623,security,model,modeling,2623,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2704,security,model,models,2704,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2715,security,model,model,2715,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2811,security,model,models,2811,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2822,security,model,model,2822,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3062,testability,understand,understand,3062,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:91,usability,error,error,91,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:331,usability,input,input,331,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:516,usability,input,input,516,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:564,usability,Error,Error,564,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:599,usability,error,error,599,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:631,usability,command,command,631,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,usability,input,input,721,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:730,usability,input,input,730,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:971,usability,input,input,971,"A summary of last trials:. 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1028,usability,input,input,1028,"ownloaded fasta file again from repository (since it had an error with chr1:4655405-4655474). 2. Ran again the WES run with the original files . 3. Got again the ""parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1396,usability,Error,Error,1396,"ediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1543,usability,User,UserWarning,1543,".bed --task 3"" Error without the more informative error message. 4. Ran again the command without num shards flag:. BIN_VERSION=""1.2.0"". ```. sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions /input/idt_capture_novogene.grch38.bed \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2978,usability,user,user,2978,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:3035,usability,error,error,3035,"utput_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir. ```. 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. INFO:tensorflow:Running local_init_op. I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op. 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2). INFO:tensorflow:Reloading EMA... I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt. I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]. ```. ```. real	0m53.331s. user	0m35.346s. sys	0m22.537s. ```. It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:23,availability,error,error,23,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:266,availability,error,error,266,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:169,deployability,fail,fails,169,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:365,deployability,resourc,resources,365,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:674,deployability,resourc,resources,674,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:704,deployability,fail,fails,704,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:365,energy efficiency,resourc,resources,365,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:436,energy efficiency,Cloud,Cloud,436,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:561,energy efficiency,Reduc,Reducing,561,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:674,energy efficiency,resourc,resources,674,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,integrability,messag,message,29,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:272,integrability,messag,messages,272,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:29,interoperability,messag,message,29,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:272,interoperability,messag,messages,272,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:414,interoperability,standard,standard-,414,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:23,performance,error,error,23,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:93,performance,memor,memory,93,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:205,performance,memor,memory,205,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:266,performance,error,error,266,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:357,performance,compute resourc,compute resources,357,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:473,performance,memor,memory,473,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:516,performance,memor,memory,516,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:674,performance,resourc,resources,674,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:169,reliability,fail,fails,169,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:618,reliability,doe,doesn,618,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:704,reliability,fail,fails,704,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:23,safety,error,error,23,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:266,safety,error,error,266,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:365,safety,resourc,resources,365,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:380,safety,test,testing,380,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:674,safety,resourc,resources,674,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:365,testability,resourc,resources,365,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:380,testability,test,testing,380,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:674,testability,resourc,resources,674,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:23,usability,error,error,23,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:93,usability,memor,memory,93,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:205,usability,memor,memory,205,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:266,usability,error,error,266,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:473,usability,memor,memory,473,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:516,usability,memor,memory,516,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:602,usability,help,help,602,"## When there isn't an error message at all:. Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails. Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:211,availability,down,downloaded,211,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked ! Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:68,performance,memor,memory,68,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked ! Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:170,performance,time,time,170,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked ! Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:68,usability,memor,memory,68,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked ! Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:291,usability,support,support,291,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked ! Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/484:205,integrability,FILTER,FILTER,205,"DeepVariant outputs all sites for which candidate variants are found, all of which had pileup images created and classified by the neural network. All candidates show up in the final VCF as `PASS` in the `FILTER` column if a het or hom-alt call was made, otherwise `RefCall`. And assuming you supply `--output_gvcf=` (see README for example) you'll get the gVCF as well. We do not have a way to output every single base on its own line.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/484
https://github.com/google/deepvariant/issues/484:138,performance,network,network,138,"DeepVariant outputs all sites for which candidate variants are found, all of which had pileup images created and classified by the neural network. All candidates show up in the final VCF as `PASS` in the `FILTER` column if a het or hom-alt call was made, otherwise `RefCall`. And assuming you supply `--output_gvcf=` (see README for example) you'll get the gVCF as well. We do not have a way to output every single base on its own line.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/484
https://github.com/google/deepvariant/issues/484:138,security,network,network,138,"DeepVariant outputs all sites for which candidate variants are found, all of which had pileup images created and classified by the neural network. All candidates show up in the final VCF as `PASS` in the `FILTER` column if a het or hom-alt call was made, otherwise `RefCall`. And assuming you supply `--output_gvcf=` (see README for example) you'll get the gVCF as well. We do not have a way to output every single base on its own line.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/484
https://github.com/google/deepvariant/issues/485:75,deployability,version,version,75,Can you include all the commands you ran? Also can you try with the latest version of DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:75,integrability,version,version,75,Can you include all the commands you ran? Also can you try with the latest version of DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:75,modifiability,version,version,75,Can you include all the commands you ran? Also can you try with the latest version of DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:24,usability,command,commands,24,Can you include all the commands you ran? Also can you try with the latest version of DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:100,availability,error,error,100,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:100,performance,error,error,100,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:100,safety,error,error,100,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:145,testability,understand,understand,145,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:100,usability,error,error,100,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:436,usability,help,help,436,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. . Could you please tell me more information about the candidate variants part? Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:96,performance,content,content,96,"For the detailed explanation of how candidates are generated please see https://www.biorxiv.org/content/10.1101/092890v6.full.pdf (starting at page 14). . Also, main code is in allelecounter.cc and variant_calling.cc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:66,availability,error,error,66,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:18,interoperability,share,share,18,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:66,performance,error,error,66,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:66,safety,error,error,66,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:66,usability,error,error,66,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/486:233,energy efficiency,optim,optimal,233,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:162,integrability,sub,substitution,162,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:175,integrability,event,events,175,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:198,integrability,event,event,198,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:229,integrability,sub,sub-optimal,229,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:125,reliability,doe,does,125,"Hi @maca8e,. Thank you for your question and for noticing this. . Yes, the way that DeepVariant generates candidate variants does not directly represent adjacent substitution events as a single MNV event. This can result in some sub-optimal representations in the VCF. We have flagged this as an area for improvement, but we have not been able to prioritize this area for awhile. I think that fixing this in the most principled way (at the level of candidate generation) would be a fair amount of work. The fastest solution would be post-calling phasing and allele reconciliation. I have not sufficiently looked into this, but I will do so and see if I have any recommendations to make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:89,security,sign,significant,89,"Hi @AndrewCarroll, . Thanks for your response and yes indeed, imagine it could take some significant reworking. Appreciate any thoughts you might have on it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:541,availability,error,errors,541,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:587,deployability,pipelin,pipeline,587,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:453,energy efficiency,predict,predictors,453,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:661,energy efficiency,predict,predict,661,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:587,integrability,pipelin,pipeline,587,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:50,interoperability,convers,conversation,50,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:677,modifiability,polymorph,polymorphisms,677,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:541,performance,error,errors,541,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:382,reliability,doe,doesn,382,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:453,safety,predict,predictors,453,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:541,safety,error,errors,541,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:661,safety,predict,predict,661,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:541,usability,error,errors,541,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:793,usability,clear,clear,793,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed. -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:128,usability,close,close,128,"Thanks all for your thoughts. Like @AndrewCarroll mentioned, we still won't be able to prioritize this in the near future. I'll close this for now, and I've opened an internal tracker for this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/487:184,deployability,releas,release,184,"Hi @edg1983 , thanks for bringing up this issue! We have already been looking into this, and have already made a few internal fixes (done by @akolesnikov) that will be out in the next release. I'm closing this issue for now. Feel free to comment or reopen if you have more questions or suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487
https://github.com/google/deepvariant/issues/488:50,usability,command,commands,50,@Suke-fudan did you retain a copy of the original commands that you ran? . Your pileup images appear to be 140 reads. This value is normally set by default to be 100 or 300. Did you possibly set this somewhere? Can you reply with the commands and flags you used?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:234,usability,command,commands,234,@Suke-fudan did you retain a copy of the original commands that you ran? . Your pileup images appear to be 140 reads. This value is normally set by default to be 100 or 300. Did you possibly set this somewhere? Can you reply with the commands and flags you used?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:56,deployability,version,version,56,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:299,deployability,version,version,299,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:474,deployability,version,version,474,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:837,deployability,version,version,837,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:56,integrability,version,version,56,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:299,integrability,version,version,299,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:474,integrability,version,version,474,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:837,integrability,version,version,837,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:56,modifiability,version,version,56,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:124,modifiability,PAC,PACBIO,124,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:299,modifiability,version,version,299,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:474,modifiability,version,version,474,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:837,modifiability,version,version,837,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:911,modifiability,PAC,PACBIO,911,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1525,safety,reme,remember,1525,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:17,usability,command,commands,17,"Yes,the original commands are as follows. #deepvariant1 version:1.1.0. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref ${ref} \. --reads ${bam} \. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz \. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz \. --num_shards 8. #whatshap phase version:1.0. whatshap phase \. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. --reference ${ref} \. ${outdir1}/${sample}.dv1.vcf.gz \. ${bam}. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \ . --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref} \. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz \. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ${ref} \. --reads_child ${outdir2}/C1_haplotagged.bam \. --reads_parent1 ${outdir2}/F1_haplotagged.bam \. --reads_parent2 ${outdir2}/M1_haplotagged.bam \. --output_vcf_child ${outdir4}/C1.output.vcf.gz \ . --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz \. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz \. --sample_name_child 'C1' \. --sample_name_parent1 'F1' \. --sample_name_parent2 'M1' \. --num_shards 8 \. --output_gvcf_child ${outdir4}/C1.g.vcf.gz \. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz \. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz \. --use_hp_information. However, I didn't remember I set that value.This thing is a little weird.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:481,availability,down,down,481,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:744,deployability,stage,stage,744,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:31,energy efficiency,model,models,31,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:612,modifiability,PAC,PACBIO,612,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:753,reliability,Doe,Does,753,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:31,security,model,models,31,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:436,usability,command,command,436,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:575,usability,command,command,575,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:. https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:. https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:. `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`. in your make_examples stage. . Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:224,availability,robust,robust,224,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:448,availability,robust,robust,448,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:414,energy efficiency,current,current,414,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:21,integrability,messag,message,21,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:21,interoperability,messag,message,21,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:224,reliability,robust,robust,224,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:448,reliability,robust,robust,448,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:224,safety,robust,robust,224,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:448,safety,robust,robust,448,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:275,usability,command,command,275,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:392,usability,command,command,392,"Oh, btw, the warning message in call_variants.py about image height might be confusing. It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:132,deployability,releas,release,132,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:455,deployability,releas,release,455,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:140,energy efficiency,model,models,140,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:289,energy efficiency,model,models,289,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:363,energy efficiency,model,models,363,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:463,energy efficiency,model,models,463,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:172,interoperability,specif,specify,172,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:273,modifiability,PAC,PACBIO,273,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:530,safety,compl,completely,530,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:140,security,model,models,140,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:289,security,model,models,289,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:363,security,model,models,363,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:463,security,model,models,463,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:530,security,compl,completely,530,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:783,deployability,version,version,783,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:546,energy efficiency,model,model-case-study,546,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:739,energy efficiency,model,model-case-study,739,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:783,integrability,version,version,783,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:138,modifiability,pac,pacbio-case-study,138,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:456,modifiability,Pac,PacBio,456,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:539,modifiability,pac,pacbio-model-case-study,539,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:732,modifiability,pac,pacbio-model-case-study,732,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:783,modifiability,version,version,783,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:351,safety,input,input,351,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:546,security,model,model-case-study,546,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:739,security,model,model-case-study,739,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:64,usability,command,commands,64,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:351,usability,input,input,351,"Thank you for your answer. The reference I ran the the original commands is https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-pacbio-case-study.md. The data I uesd is HiFi reads of a family of four. The --use_hp_information arg makes use of a phased reads, thus allowing a further improvement of the accuracy. In order to use this feature input BAM files have to be phased. For the detailed description on how to do that please see DeepVariant PacBio case study.https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. The ""deepvariant1-whatshap phase-whatshap haplotag"" was used to generate phased bam file.(reference:https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md). Should I use a higher version of DeepVariant(1.2.0) or regenerate the phased bam file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:95,deployability,version,version,95,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:136,deployability,version,version,136,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:95,integrability,version,version,95,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:136,integrability,version,version,136,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:95,modifiability,version,version,95,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:136,modifiability,version,version,136,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:159,performance,perform,perform,159,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:187,performance,perform,performed,187,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:159,usability,perform,perform,159,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:187,usability,perform,performed,187,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:75,deployability,version,version,75,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:310,deployability,version,version,310,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:521,deployability,version,version,521,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:917,deployability,version,version,917,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:75,integrability,version,version,75,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:310,integrability,version,version,310,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:521,integrability,version,version,521,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:917,integrability,version,version,917,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:75,modifiability,version,version,75,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:141,modifiability,PAC,PACBIO,141,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:310,modifiability,version,version,310,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:521,modifiability,version,version,521,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:917,modifiability,version,version,917,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:989,modifiability,PAC,PACBIO,989,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:19,usability,help,help,19,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:39,usability,command,commands,39,"Thank you for your help!So,the correct commands may be:. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag \. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz \. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1618,availability,error,errors,1618,"haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1684,availability,error,error,1684,"e ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1985,availability,error,error,1985,". --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2480,availability,error,error,2480,"r4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2975,availability,error,error,2975,"th. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6570,availability,reliab,reliable,6570,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:18,deployability,version,version,18,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:91,deployability,version,version,91,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:326,deployability,version,version,326,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:537,deployability,version,version,537,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:931,deployability,version,version,931,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1582,deployability,log,log,1582,"deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1609,deployability,contain,contains,1609,"hatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepV",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1671,deployability,log,log,1671,"bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1923,deployability,version,version,1923,"rio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of Dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2418,deployability,version,version,2418,"f_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2913,deployability,version,version,2913,"s and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3324,deployability,log,log,3324,"rd in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3491,deployability,modul,module,3491," appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6111,deployability,modul,module,6111,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6463,deployability,fail,failed,6463,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6518,deployability,fail,failed,6518,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1877,energy efficiency,model,model,1877,"m. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2146,energy efficiency,model,model,2146,"plotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2372,energy efficiency,model,model,2372,"name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2641,energy efficiency,model,model,2641," have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2867,energy efficiency,model,model,2867,"at the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3136,energy efficiency,model,model,3136," use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5059,energy efficiency,predict,predictions,5059,"ariants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:18,integrability,version,version,18,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:91,integrability,version,version,91,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:326,integrability,version,version,326,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:537,integrability,version,version,537,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:931,integrability,version,version,931,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1923,integrability,version,version,1923,"rio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of Dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2418,integrability,version,version,2418,"f_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2913,integrability,version,version,2913,"s and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1792,interoperability,standard,standard,1792,"epvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in De",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1825,interoperability,standard,standard,1825," samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in De",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2022,interoperability,mismatch,mismatched,2022,"reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched beca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2287,interoperability,standard,standard,2287,"4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2320,interoperability,standard,standard,2320,"e_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2517,interoperability,mismatch,mismatched,2517,"${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run()",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2782,interoperability,standard,standard,2782," 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2815,interoperability,standard,standard,2815," 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3012,interoperability,mismatch,mismatched,3012," are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3587,interoperability,platform,platform,3587,"_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:18,modifiability,version,version,18,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:91,modifiability,version,version,91,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:157,modifiability,PAC,PACBIO,157,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:326,modifiability,version,version,326,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:537,modifiability,version,version,537,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:931,modifiability,version,version,931,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1003,modifiability,PAC,PACBIO,1003," the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1908,modifiability,paramet,parameters,1908,"alling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1923,modifiability,version,version,1923,"rio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of Dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2175,modifiability,paramet,parameter,2175,"ld ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. rais",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2403,modifiability,paramet,parameters,2403,". --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2418,modifiability,version,version,2418,"f_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2670,modifiability,paramet,parameter,2670,"|grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2898,modifiability,paramet,parameters,2898,"same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2913,modifiability,version,version,2913,"s and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3165,modifiability,paramet,parameter,3165,"same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3491,modifiability,modul,module,3491," appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3560,modifiability,pac,packages,3560,"te that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6111,modifiability,modul,module,6111,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6171,modifiability,pac,packages,6171,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6271,modifiability,pac,packages,6271,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1618,performance,error,errors,1618,"haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1684,performance,error,error,1684,"e ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1985,performance,error,error,1985,". --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2480,performance,error,error,2480,"r4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2975,performance,error,error,2975,"th. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6463,reliability,fail,failed,6463,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6518,reliability,fail,failed,6518,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6570,reliability,reliab,reliable,6570,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1582,safety,log,log,1582,"deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1618,safety,error,errors,1618,"haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1671,safety,log,log,1671,"bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1684,safety,error,error,1684,"e ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1768,safety,input,input,1768,". ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1985,safety,error,error,1985,". --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2263,safety,input,input,2263,"tput_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2480,safety,error,error,2480,"r4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2758,safety,input,input,2758,"of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2975,safety,error,error,2975,"th. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3231,safety,sanit,sanity,3231,"ts.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in me",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3297,safety,sanit,sanity,3297,"eepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3324,safety,log,log,3324,"rd in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3491,safety,modul,module,3491," appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5059,safety,predict,predictions,5059,"ariants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5289,safety,sanit,sanity,5289,"none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5355,safety,sanit,sanity,5355,"files/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6111,safety,modul,module,6111,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6419,safety,Except,Exception,6419,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6474,safety,Except,Exception,6474,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1582,security,log,log,1582,"deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1671,security,log,log,1671,"bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1877,security,model,model,1877,"m. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2146,security,model,model,2146,"plotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2372,security,model,model,2372,"name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2641,security,model,model,2641," have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2867,security,model,model,2867,"at the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3136,security,model,model,3136," use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3231,security,sanit,sanity,3231,"ts.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in me",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3297,security,sanit,sanity,3297,"eepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3324,security,log,log,3324,"rd in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5289,security,sanit,sanity,5289,"none(variant_iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5355,security,sanit,sanity,5355,"files/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1582,testability,log,log,1582,"deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1671,testability,log,log,1671,"bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3324,testability,log,log,3324,"rd in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:3331,testability,Trace,Traceback,3331,"Trio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main. merge_and_write_variants_and_nonvariants(variant_generator,. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/TMP_DIR/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:6007,testability,Trace,Traceback,6007,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:61,usability,command,commands,61,"I used the latest version of DeepVariant (1.2.0) and ran the commands：\. #①. #deepvariant1 version:1.2.0. /opt/deepvariant/bin/run_deepvariant. --model_type PACBIO. --ref ${ref}. --reads ${bam}. --output_vcf ${outdir1}/${sample}.dv1.vcf.gz. --output_gvcf ${outdir1}/${sample}.dv1.g.vcf.gz. --num_shards 8. #②. #whatshap phase version:1.0. whatshap phase. --output ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. --reference ${ref}. ${outdir1}/${sample}.dv1.vcf.gz. ${bam}. #③generate phased bam to use DeepTrio1.2.0. #whatshap haplotag version:1.0. tabix -p vcf ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. whatshap haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1618,usability,error,errors,1618,"haplotag. --output ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1684,usability,error,error,1684,"e ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam"". --reference ${ref}. ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1768,usability,input,input,1768,". ${outdir2}/${sample}_deepvariant1.phased.vcf.gz. ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling. #DeepTrio version:1.2.0. /opt/deepvariant/bin/deeptrio/run_deeptrio. --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1985,usability,error,error,1985,". --model_type PACBIO. --ref ${ref}. --reads_child ${outdir2}/C1_haplotagged.bam. --reads_parent1 ${outdir2}/F1_haplotagged.bam. --reads_parent2 ${outdir2}/M1_haplotagged.bam. --output_vcf_child ${outdir4}/C1.output.vcf.gz. --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz. --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2263,usability,input,input,2263,"tput_vcf_parent2 ${outdir4}/M1.output.vcf.gz. --sample_name_child 'C1'. --sample_name_parent1 'F1'. --sample_name_parent2 'M1'. --num_shards 8. --output_gvcf_child ${outdir4}/C1.g.vcf.gz. --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2480,usability,error,error,2480,"r4}/F1.g.vcf.gz. --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz. --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated). cat log |grep -i error. W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2758,usability,input,input,2758,"of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2975,usability,error,error,2975,"th. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**. Traceback (most recent call last):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5385,usability,user,user,5385,"deepvariant/postprocess_variants.py"", line 998, in next_or_none. return next(iterable). File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5929,usability,user,user,5929,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:5976,usability,user,user,5976,"ing_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s. user	14m3.211s. sys	0m20.620s. I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes. I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader. I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes. I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s. user	43m7.954s. sys	1m1.029s. real	47m46.669s. user	45m31.656s. sys	1m0.352s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:300,energy efficiency,model,models,300,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:406,energy efficiency,model,model,406,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:104,interoperability,standard,standard,104,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:137,interoperability,standard,standard,137,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:572,interoperability,standard,standard,572,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:293,modifiability,PAC,PACBIO,293,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:504,modifiability,PAC,PACBIO,504,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:80,safety,input,input,80,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:300,security,model,models,300,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:406,security,model,model,406,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:203,testability,plan,plan,203,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:80,usability,input,input,80,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:262,usability,clear,clear,262,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:. - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total). - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:128,availability,error,error,128,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:206,availability,error,errors,206,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:560,availability,error,errors,560,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:465,deployability,fail,failed,465,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:520,deployability,fail,failed,520,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:128,performance,error,error,128,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:206,performance,error,errors,206,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:560,performance,error,errors,560,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:465,reliability,fail,failed,465,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:520,reliability,fail,failed,520,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:128,safety,error,error,128,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:206,safety,error,errors,206,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:337,safety,sanit,sanity,337,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:401,safety,sanit,sanity,401,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:421,safety,Except,Exception,421,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:476,safety,Except,Exception,476,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:560,safety,error,errors,560,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:337,security,sanit,sanity,337,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:401,security,sanit,sanity,401,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:16,usability,help,help,16,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:128,usability,error,error,128,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:206,usability,error,errors,206,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:560,usability,error,errors,560,"Thanks for your help! I can ignore the warning about ""height"". However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:. **raise ValueError('call_variants_outputs did not pass sanity check.'). ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'). Exception: One or more postprocess_variants failed.**. How should I deal with these errors？.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:46,availability,error,error,46,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:192,availability,error,error,192,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:287,availability,error,error,287,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:46,performance,error,error,46,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:192,performance,error,error,192,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:287,performance,error,error,287,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:46,safety,error,error,46,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:192,safety,error,error,192,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:287,safety,error,error,287,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:151,security,ident,identify,151,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:46,usability,error,error,46,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:192,usability,error,error,192,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:287,usability,error,error,287,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome? Are you able to identify which region of the genome this error is occurring at? Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:102,availability,error,error,102,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:108,integrability,messag,message,108,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:108,interoperability,messag,message,108,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:102,performance,error,error,102,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:102,safety,error,error,102,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:24,usability,help,help,24,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:56,usability,command,commands,56,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:102,usability,error,error,102,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:28,deployability,updat,update,28,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:104,deployability,releas,release,104,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:190,deployability,updat,update,190,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:201,deployability,log,logging,201,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:159,energy efficiency,model,model,159,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:83,integrability,messag,message,83,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:209,integrability,messag,message,209,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:83,interoperability,messag,message,83,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:209,interoperability,messag,message,209,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:28,safety,updat,update,28,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:134,safety,input,input,134,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:190,safety,updat,update,190,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:201,safety,log,logging,201,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:28,security,updat,update,28,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:159,security,model,model,159,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:190,security,updat,update,190,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:201,security,log,logging,201,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:116,testability,plan,plan,116,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:201,testability,log,logging,201,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:134,usability,input,input,134,"Thanks @Suke-fudan for your update. And thanks for reporting the confusing warning message. In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/489:349,deployability,toolchain,toolchain,349,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:437,deployability,INSTAL,INSTALL,437,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:519,deployability,modul,modules,519,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:566,deployability,build,build,566,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:603,deployability,instal,installation,603,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:702,deployability,updat,updating,702,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:62,energy efficiency,current,currently,62,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:171,integrability,Configur,Configure,171,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:193,integrability,repositor,repository,193,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:288,integrability,repositor,repository,288,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:193,interoperability,repositor,repository,193,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:288,interoperability,repositor,repository,288,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:171,modifiability,Configur,Configure,171,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:519,modifiability,modul,modules,519,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:519,safety,modul,modules,519,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:702,safety,updat,updating,702,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:784,safety,review,review,784,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:171,security,Configur,Configure,171,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:189,security,apt,apt,189,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:223,security,apt,apt,223,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:260,security,apt,apt-key,260,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:284,security,apt,apt-repository,284,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:311,security,apt,apt,311,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:599,security,apt,apt,599,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:666,security,hack,hack,666,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:702,security,updat,updating,702,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:740,security,hack,hack,740,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:784,testability,review,review,784,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:349,usability,tool,toolchain,349,"Thanks @Stikus , I noticed this and was just looking at it! I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:. ```. # Configure LLVM 11 apt repository. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \. add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". ```. and then , right in front of the line of `./INSTALL.sh` , add this line:. ```. sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake. ```. Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:16,deployability,patch,patch,16,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:173,modifiability,maintain,maintainers,173,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:356,reliability,doe,doesn,356,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:16,safety,patch,patch,16,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:173,safety,maintain,maintainers,173,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:16,security,patch,patch,16,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:57,reliability,doe,doesn,57,I've tested your solution - it works on Ubuntu 20.04 but doesn't work on Ubuntu 18.04. Still looking for solution,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5,safety,test,tested,5,I've tested your solution - it works on Ubuntu 20.04 but doesn't work on Ubuntu 18.04. Still looking for solution,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5,testability,test,tested,5,I've tested your solution - it works on Ubuntu 20.04 but doesn't work on Ubuntu 18.04. Still looking for solution,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:195,deployability,instal,installation,195,Finally fixed it - looks like fix was on LLVM side:. ![image](https://user-images.githubusercontent.com/41360525/137140712-84f012cb-b7b6-4d7a-a08b-22fd20ec93ed.png). I have no changes in our old installation and all working again (except pinning `jsonschema==3.2.0` as you've done [here](https://github.com/google/deepvariant/commit/fd02fa3ab8fa1d161e23d10c9931641d7ab1dcad) ),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:231,safety,except,except,231,Finally fixed it - looks like fix was on LLVM side:. ![image](https://user-images.githubusercontent.com/41360525/137140712-84f012cb-b7b6-4d7a-a08b-22fd20ec93ed.png). I have no changes in our old installation and all working again (except pinning `jsonschema==3.2.0` as you've done [here](https://github.com/google/deepvariant/commit/fd02fa3ab8fa1d161e23d10c9931641d7ab1dcad) ),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:70,usability,user,user-images,70,Finally fixed it - looks like fix was on LLVM side:. ![image](https://user-images.githubusercontent.com/41360525/137140712-84f012cb-b7b6-4d7a-a08b-22fd20ec93ed.png). I have no changes in our old installation and all working again (except pinning `jsonschema==3.2.0` as you've done [here](https://github.com/google/deepvariant/commit/fd02fa3ab8fa1d161e23d10c9931641d7ab1dcad) ),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:245,deployability,updat,updated,245,Thanks @Stikus . Glad it's working. Internally I've made another change similar to https://github.com/google/clif/commit/83c7941aae9f01a575adc719b1baed3c6607b8f4 which should work for 18.04. I haven't pushed that to GitHub but will hopefully be updated to our r1.2 as well.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:245,safety,updat,updated,245,Thanks @Stikus . Glad it's working. Internally I've made another change similar to https://github.com/google/clif/commit/83c7941aae9f01a575adc719b1baed3c6607b8f4 which should work for 18.04. I haven't pushed that to GitHub but will hopefully be updated to our r1.2 as well.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:245,security,updat,updated,245,Thanks @Stikus . Glad it's working. Internally I've made another change similar to https://github.com/google/clif/commit/83c7941aae9f01a575adc719b1baed3c6607b8f4 which should work for 18.04. I haven't pushed that to GitHub but will hopefully be updated to our r1.2 as well.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/490:99,energy efficiency,alloc,allocation,99,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:484,energy efficiency,model,model-case-study,484,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:646,energy efficiency,model,model,646,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:43,integrability,messag,message,43,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:629,integrability,pub,published,629,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:43,interoperability,messag,message,43,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:204,modifiability,Pac,PacBio,204,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:356,modifiability,pac,pacbio,356,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:477,modifiability,pac,pacbio-model-case-study,477,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:639,modifiability,Pac,PacBio,639,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:92,performance,memor,memory,92,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:484,security,model,model-case-study,484,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:646,security,model,model,646,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:92,usability,memor,memory,92,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:268,usability,help,helps,268,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:402,usability,workflow,workflow,402,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:669,usability,workflow,workflow,669,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem. 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md. I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:98,energy efficiency,alloc,allocation,98,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:221,modifiability,Pac,PacBio,221,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:384,modifiability,Pac,Pacbio,384,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:91,performance,memor,memory,91,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:91,usability,memor,memory,91,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:139,usability,workflow,workflow,139,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:247,usability,confirm,confirm,247,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:318,usability,workflow,workflow,318,"Hi @MariaNattestad,. Thank you for your answer. So both issues are one single problem with memory allocation. I haven't tried the WhatsHap workflow for a while now (did it on some data maybe a year ago) but I will do the PacBio case study just to confirm that it works. I have been using the PEPPER-Margin-DeepVariant workflow on corrected ONT data with great success so far (in both Pacbio and ONT mode) without running into this problem. Ultimately, the phasing is just so much faster with PEPPER-Margin than WhatsHap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:175,energy efficiency,current,current,175,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:392,integrability,pub,public,392,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:443,interoperability,share,share,443,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:124,modifiability,Pac,PacBio,124,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:105,performance,memor,memory,105,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:289,safety,input,input,289,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:105,usability,memor,memory,105,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:289,usability,input,input,289,"Hi @GuillaumeHolley ,. I'm not sure if this is related, but one separate issue we've noticed before with memory is: If your PacBio HiFi BAM has a lot of extra auxiliary tags, current DeepVariant code will try to parse all of them, and can sometimes run OOM as a result. Can you check your input BAM and see if that could be the case? If it's not that, I wonder if you can reproduce this on a public BAM (process it through PEPPER-Margin ) and share that with us, so we can take a look? Also adding @williamrowell in case you have seen this before.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:325,usability,workflow,workflow,325,"Hi @pichuan,. You might actually be onto something. I have about 15 tags per read which is not that much but some of them seem to be very very long. In particular, tags `fi`, `fp`, `ri` and `rp` have really long lists of comma-separated numbers (those tags are described [here](https://ccs.how/faq/bam-output.html). Maybe my workflow from the SMRT cells is incorrect. I use a tool named `extracthifi` to get the reads and `pbmm2` to map those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:376,usability,tool,tool,376,"Hi @pichuan,. You might actually be onto something. I have about 15 tags per read which is not that much but some of them seem to be very very long. In particular, tags `fi`, `fp`, `ri` and `rp` have really long lists of comma-separated numbers (those tags are described [here](https://ccs.how/faq/bam-output.html). Maybe my workflow from the SMRT cells is incorrect. I use a tool named `extracthifi` to get the reads and `pbmm2` to map those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:303,availability,error,errors,303,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:365,availability,error,errors,365,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:303,performance,error,errors,303,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:365,performance,error,errors,365,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:303,safety,error,errors,303,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:365,safety,error,errors,365,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:467,safety,input,input,467,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:303,usability,error,errors,303,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:365,usability,error,errors,365,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:467,usability,input,input,467,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:. `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:58,availability,error,error,58,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:149,availability,error,error,149,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:292,availability,error,error,292,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:370,deployability,pipelin,pipeline,370,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:370,integrability,pipelin,pipeline,370,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:58,performance,error,error,58,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:149,performance,error,error,149,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:292,performance,error,error,292,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:58,safety,error,error,58,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:149,safety,error,error,149,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:175,safety,test,test,175,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:292,safety,error,error,292,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:94,security,ident,identified,94,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:175,testability,test,test,175,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:58,usability,error,error,58,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:149,usability,error,error,149,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:292,usability,error,error,292,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:12,deployability,releas,release,12,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:39,deployability,updat,update,39,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:81,performance,memor,memory,81,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:39,safety,updat,update,39,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:39,security,updat,update,39,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:81,usability,memor,memory,81,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:106,energy efficiency,current,currently,106,"Wow, thank you @MariaNattestad @pichuan @williamrowell @kishwarshafin, this all went really quickly. I am currently in the process of generating {fi,fp,ri,rp}-tagless BAM files and will rerun DeepVariant on those. In the meantime, I will close this issue as I am fairly confident you found the solution to the problem. Thank you again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:238,usability,close,close,238,"Wow, thank you @MariaNattestad @pichuan @williamrowell @kishwarshafin, this all went really quickly. I am currently in the process of generating {fi,fp,ri,rp}-tagless BAM files and will rerun DeepVariant on those. In the meantime, I will close this issue as I am fairly confident you found the solution to the problem. Thank you again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:8,usability,confirm,confirm,8,Just to confirm: removing the tags worked. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/491:121,interoperability,specif,specific,121,@akolesnikov on our team will look at this and reply later. Also tagging @kishwarshafin in case some of these are PEPPER specific.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20,security,team,team,20,@akolesnikov on our team will look at this and reply later. Also tagging @kishwarshafin in case some of these are PEPPER specific.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15,deployability,updat,update,15,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:28,energy efficiency,alloc,allocated,28,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:44,performance,disk,disk,44,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15,safety,updat,update,15,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15,security,updat,update,15,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:113,testability,understand,understanding,113,"Thanks you! An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:58,availability,avail,available,58,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:49,integrability,pub,publicly,49,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:58,reliability,availab,available,58,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:58,safety,avail,available,58,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:58,security,availab,available,58,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:491,energy efficiency,load,loads,491,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:89,integrability,sub,subset,89,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:491,performance,load,loads,491,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,testability,coverag,coverage,62,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:423,testability,coverag,coverage,423,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:214,usability,behavi,behavior,214,"Unfortunately, it's not... Providing what I can:. It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:. ```. chr10+chr14-p. chr4-p+chr5-p_chr11-p. chr7-q+chr16-q. ```. all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:150,deployability,pipelin,pipeline,150,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:33,integrability,sub,subsampling,33,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:150,integrability,pipelin,pipeline,150,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:96,testability,coverag,coverage,96,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:18,integrability,sub,sub-sampling,18,"Good to know that sub-sampling trick. The N50 is 15695, which doesn't look abnormal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,reliability,doe,doesn,62,"Good to know that sub-sampling trick. The N50 is 15695, which doesn't look abnormal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:329,availability,failur,failure,329,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:329,deployability,fail,failure,329,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:346,deployability,log,logging,346,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:492,integrability,sub,submitted,492,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:26,performance,disk,disk,26,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:79,performance,memor,memory,79,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:329,performance,failur,failure,329,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:397,performance,memor,memory,397,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:468,performance,parallel,parallel,468,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:578,performance,memor,memory,578,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:329,reliability,fail,failure,329,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:346,safety,log,logging,346,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:346,security,log,logging,346,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:346,testability,log,logging,346,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:79,usability,memor,memory,79,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:121,usability,command,command,121,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:337,usability,stop,stop,337,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:363,usability,command,command,363,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:397,usability,memor,memory,397,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:436,usability,command,command,436,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:502,usability,command,command,502,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:578,usability,memor,memory,578,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:619,usability,help,helps,619,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:54,availability,error,errors,54,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:141,availability,error,error,141,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:186,availability,error,error,186,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:227,availability,error,error,227,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:295,availability,error,error,295,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:583,deployability,resourc,resources,583,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:583,energy efficiency,resourc,resources,583,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:593,energy efficiency,alloc,allocated,593,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:47,performance,memor,memory,47,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:54,performance,error,errors,54,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:141,performance,error,error,141,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:186,performance,error,error,186,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:209,performance,memor,memory,209,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:227,performance,error,error,227,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:251,performance,disk,disk,251,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:295,performance,error,error,295,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:433,performance,memor,memory,433,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:583,performance,resourc,resources,583,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:54,safety,error,errors,54,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:141,safety,error,error,141,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:186,safety,error,error,186,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:227,safety,error,error,227,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:295,safety,error,error,295,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:583,safety,resourc,resources,583,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:104,testability,coverag,coverage,104,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:508,testability,coverag,coverage,508,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:583,testability,resourc,resources,583,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:47,usability,memor,memory,47,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:54,usability,error,errors,54,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:141,usability,error,error,141,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:174,usability,experien,experience,174,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:186,usability,error,error,186,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:199,usability,indicat,indicates,199,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:209,usability,memor,memory,209,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:227,usability,error,error,227,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:241,usability,indicat,indicates,241,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:295,usability,error,error,295,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:389,usability,help,helpful,389,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:433,usability,memor,memory,433,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:469,usability,workflow,workflow,469,"Thanks, Paul! I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll . * try with increased the memory. * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and . * also use that for collecting data in tuning the resources allocated to DV. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,availability,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,deployability,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,energy efficiency,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:168,integrability,filter,filter,168,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:85,performance,disk,disk,85,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:188,performance,disk,disk,188,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,reliability,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,safety,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:77,testability,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:54,usability,command,command,54,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:219,usability,help,helps,219,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3,deployability,updat,update,3,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:177,deployability,pipelin,pipeline,177,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:215,deployability,pipelin,pipeline,215,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:228,deployability,build,building,228,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:351,deployability,pipelin,pipeline,351,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:444,deployability,resourc,resources,444,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:444,energy efficiency,resourc,resources,444,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:177,integrability,pipelin,pipeline,177,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:215,integrability,pipelin,pipeline,215,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:351,integrability,pipelin,pipeline,351,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:39,performance,memor,memory,39,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:50,performance,disk,disk,50,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:141,performance,time,time,141,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:444,performance,resourc,resources,444,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3,safety,updat,update,3,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:444,safety,resourc,resources,444,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3,security,updat,update,3,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:444,testability,resourc,resources,444,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:39,usability,memor,memory,39,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:336,usability,prototyp,prototyped,336,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:463,usability,user,user-images,463,"An update:. I've tried increasing both memory and disk, and it has worked! Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:210,deployability,resourc,resource,210,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:435,deployability,version,version,435,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:210,energy efficiency,resourc,resource,210,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:280,energy efficiency,cpu,cpu,280,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:296,energy efficiency,profil,profiles,296,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:327,energy efficiency,profil,profile,327,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:435,integrability,version,version,435,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:100,modifiability,layer,layers,100,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:435,modifiability,version,version,435,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:210,performance,resourc,resource,210,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:273,performance,memor,memory,273,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:280,performance,cpu,cpu,280,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:296,performance,profil,profiles,296,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:327,performance,profil,profile,327,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:525,performance,memor,memory,525,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:532,performance,disk,disk,532,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:210,safety,resourc,resource,210,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:401,safety,input,input,401,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:426,security,modif,modified,426,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:210,testability,resourc,resource,210,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:394,testability,simpl,simple,394,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:565,testability,trace,trace,565,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:65,usability,visual,visualization,65,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:273,usability,memor,memory,273,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:394,usability,simpl,simple,394,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:401,usability,input,input,401,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:525,usability,memor,memory,525,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,deployability,log,logging,62,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:149,deployability,releas,release,149,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:554,deployability,releas,release,554,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:590,deployability,log,log,590,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,safety,log,logging,62,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:453,safety,test,test,453,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:590,safety,log,log,590,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,security,log,logging,62,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:590,security,log,log,590,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:62,testability,log,logging,62,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:453,testability,test,test,453,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:590,testability,log,log,590,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. . I've made two changes internally (which will come out in the next release):. 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead. 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:53,deployability,log,log,53,@pichuan thanks for the work! I can confirm that the log file is indeed much smaller now with v1.3.0. And it's running faster. Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:53,safety,log,log,53,@pichuan thanks for the work! I can confirm that the log file is indeed much smaller now with v1.3.0. And it's running faster. Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:53,security,log,log,53,@pichuan thanks for the work! I can confirm that the log file is indeed much smaller now with v1.3.0. And it's running faster. Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:53,testability,log,log,53,@pichuan thanks for the work! I can confirm that the log file is indeed much smaller now with v1.3.0. And it's running faster. Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36,usability,confirm,confirm,36,@pichuan thanks for the work! I can confirm that the log file is indeed much smaller now with v1.3.0. And it's running faster. Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/492:64,safety,input,input,64,What is the index filename? By convention it is expected to be /input/G1_hap2.bam.bai,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:64,usability,input,input,64,What is the index filename? By convention it is expected to be /input/G1_hap2.bam.bai,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:105,availability,error,error,105,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:518,deployability,Fail,Failed,518,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:738,deployability,modul,module,738,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2357,deployability,Fail,Failed,2357,"p/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2577,deployability,modul,module,2577,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4196,deployability,Fail,Failed,4196,"p/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4416,deployability,modul,module,4416,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6035,deployability,Fail,Failed,6035,"p/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6255,deployability,modul,module,6255,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7872,deployability,fail,failed,7872,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:738,modifiability,modul,module,738,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2577,modifiability,modul,module,2577,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4416,modifiability,modul,module,4416,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6255,modifiability,modul,module,6255,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:105,performance,error,error,105,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7853,performance,parallel,parallel,7853,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:518,reliability,Fail,Failed,518,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2357,reliability,Fail,Failed,2357,"p/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4196,reliability,Fail,Failed,4196,"p/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6035,reliability,Fail,Failed,6035,"p/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7872,reliability,fail,failed,7872,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:105,safety,error,error,105,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:199,safety,input,input,199,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:383,safety,input,input,383,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:423,safety,input,input,423,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:540,safety,input,input,540,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:738,safety,modul,module,738,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2315,safety,input,input,2315,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2379,safety,input,input,2379,"aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2577,safety,modul,module,2577,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4154,safety,input,input,4154,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4218,safety,input,input,4218,"4x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4416,safety,modul,module,4416,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5993,safety,input,input,5993,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6057,safety,input,input,6057,"uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6255,safety,modul,module,6255,"s/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7832,safety,input,input,7832,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7938,safety,input,input,7938,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7975,safety,input,input,7975,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:590,testability,Trace,Traceback,590,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2429,testability,Trace,Traceback,2429,"_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4268,testability,Trace,Traceback,4268,"_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6107,testability,Trace,Traceback,6107,"_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:48,usability,command,command,48,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:105,usability,error,error,105,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:124,usability,command,command,124,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:199,usability,input,input,199,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:383,usability,input,input,383,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:423,usability,input,input,423,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:540,usability,input,input,540,"I ran it with a different dataset with the same command just changed the files, I am getting a different error. This is the command I have. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \. -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1-hifi.contigs.fasta \. --reads=/input/G1_sorted.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2315,usability,input,input,2315,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2379,usability,input,input,2379,"aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4154,usability,input,input,4154,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4218,usability,input,input,4218,"4x8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_7p9q24x8/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5993,usability,input,input,5993,"_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_fl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:6057,usability,input,input,6057,"uwf/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_fmpe4uwf/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7832,usability,input,input,7832,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7938,usability,input,input,7938,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7975,usability,input,input,7975,"ut/G1_sorted.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u7og4tn6/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 223, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open /input/G1_sorted.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1-hifi.contigs.fasta --reads /input/G1_sorted.bam --examples /tmp/tmpe6hg2fef/make_examples.tfrecord@4.gz --task 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:89,energy efficiency,model,model,89,"Did you finally solve this issue? I've encountered the same problem when running the WGS model. It says that it cannot query without and index and that it cannot find the index file for the BAM file, when it is in the same directory as the bam file and it has the same name with the extension .bai.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:283,modifiability,extens,extension,283,"Did you finally solve this issue? I've encountered the same problem when running the WGS model. It says that it cannot query without and index and that it cannot find the index file for the BAM file, when it is in the same directory as the bam file and it has the same name with the extension .bai.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:89,security,model,model,89,"Did you finally solve this issue? I've encountered the same problem when running the WGS model. It says that it cannot query without and index and that it cannot find the index file for the BAM file, when it is in the same directory as the bam file and it has the same name with the extension .bai.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:26,interoperability,share,share,26,"Hi @luciamayorf , can you share your command? And, can you quickly test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md and see if that worked for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:67,safety,test,test,67,"Hi @luciamayorf , can you share your command? And, can you quickly test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md and see if that worked for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:67,testability,test,test,67,"Hi @luciamayorf , can you share your command? And, can you quickly test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md and see if that worked for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:37,usability,command,command,37,"Hi @luciamayorf , can you share your command? And, can you quickly test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md and see if that worked for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/493:178,energy efficiency,frequenc,frequency,178,"Is it possible the genome is tetraploid? The distribution looks just about perfectly like I would expect if there are 4 copies of each sequence, since the middle peaks of allele frequency are around 0.25 and 0.75. It's funny there isn't a peak at 0.5 then, so I'm curious what you think is going on here. DeepVariant assumes the genome is diploid and is only trained on human autosomes, so I want to make sure you are aware of that when interpreting the results!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:45,interoperability,distribut,distribution,45,"Is it possible the genome is tetraploid? The distribution looks just about perfectly like I would expect if there are 4 copies of each sequence, since the middle peaks of allele frequency are around 0.25 and 0.75. It's funny there isn't a peak at 0.5 then, so I'm curious what you think is going on here. DeepVariant assumes the genome is diploid and is only trained on human autosomes, so I want to make sure you are aware of that when interpreting the results!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:104,availability,consist,consistent,104,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:389,availability,consist,consistent,389,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:593,deployability,observ,observation,593,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:504,integrability,sub,sub-population,504,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:335,reliability,doe,does,335,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:593,testability,observ,observation,593,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:104,usability,consist,consistent,104,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:389,usability,consist,consistent,389,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:535,usability,close,close,535,"Hi @madhubioinfo . Maria's explanation is very interesting. If this is a plausible explanation, this is consistent with one grandparent being much more divergent from the reference genome. So most of the variants are contributed by this grandparent (and also explains why variants are either at 0.25 or 0.75. If this is a strain which does not undergo much recombination, it would also be consistent that one haploid genome is more diverged and this is either a tetraploid, or pooled sequencing with one sub-population at suspiciously close to 25% of the bulk size. This is a very interesting observation. I am curious for your thoughts on this possibility.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:729,energy efficiency,frequenc,frequencies,729,"Bit more background information. The genome used here is a haploid ( dikaryon ) fungi. The assembled genome is from a Hifasm assembler. Haploid information is confirmed with flow cytometry experiment. It's haploid for sure, and one of the two parental genotypes is known to represent 80% more abundant. So the question is why is the first peak not at the same level as the one at 80 given that they are heterozygous. . So one parent is represent 0.8% of the genome present in the cell and the other is 0.2. We know that using molecular methods - one genotypes = 80% of the nuclei coexisting in the cell. What we don't know is why do the bi-allelic freqs have different sizes. It looks like there is a small shared portion of the frequencies that are shared, and the rest of the peak at 0.2 represents something else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:707,interoperability,share,shared,707,"Bit more background information. The genome used here is a haploid ( dikaryon ) fungi. The assembled genome is from a Hifasm assembler. Haploid information is confirmed with flow cytometry experiment. It's haploid for sure, and one of the two parental genotypes is known to represent 80% more abundant. So the question is why is the first peak not at the same level as the one at 80 given that they are heterozygous. . So one parent is represent 0.8% of the genome present in the cell and the other is 0.2. We know that using molecular methods - one genotypes = 80% of the nuclei coexisting in the cell. What we don't know is why do the bi-allelic freqs have different sizes. It looks like there is a small shared portion of the frequencies that are shared, and the rest of the peak at 0.2 represents something else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:750,interoperability,share,shared,750,"Bit more background information. The genome used here is a haploid ( dikaryon ) fungi. The assembled genome is from a Hifasm assembler. Haploid information is confirmed with flow cytometry experiment. It's haploid for sure, and one of the two parental genotypes is known to represent 80% more abundant. So the question is why is the first peak not at the same level as the one at 80 given that they are heterozygous. . So one parent is represent 0.8% of the genome present in the cell and the other is 0.2. We know that using molecular methods - one genotypes = 80% of the nuclei coexisting in the cell. What we don't know is why do the bi-allelic freqs have different sizes. It looks like there is a small shared portion of the frequencies that are shared, and the rest of the peak at 0.2 represents something else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:159,usability,confirm,confirmed,159,"Bit more background information. The genome used here is a haploid ( dikaryon ) fungi. The assembled genome is from a Hifasm assembler. Haploid information is confirmed with flow cytometry experiment. It's haploid for sure, and one of the two parental genotypes is known to represent 80% more abundant. So the question is why is the first peak not at the same level as the one at 80 given that they are heterozygous. . So one parent is represent 0.8% of the genome present in the cell and the other is 0.2. We know that using molecular methods - one genotypes = 80% of the nuclei coexisting in the cell. What we don't know is why do the bi-allelic freqs have different sizes. It looks like there is a small shared portion of the frequencies that are shared, and the rest of the peak at 0.2 represents something else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:90,availability,consist,consistent,90,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:78,deployability,observ,observation,78,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:432,energy efficiency,draw,draw,432,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:799,interoperability,share,shared,799,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:192,performance,content,content,192,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:921,performance,content,content,921,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:78,testability,observ,observation,78,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:90,usability,consist,consistent,90,"Hi @madhubioinfo . Thank you for the additional information. I'll propose one observation consistent with what you've provided. Of the two genomes, the one representing 20% of the net genomic content is much more diverged from the reference genome you are using in comparison to the other parent. The amount of divergence determines how many variants one would expect relative to a reference. If this hypothesis is correct, I would draw the following conclusions:. 1. The less abundant genome is more diverged relative to the reference. 2. The more abundant genome has one 1,000 - 2,000 positions that differ from the reference. . 3. (Just eyeballing from the graph of absolute # of HET and HOM calls), among the positions that do vary in the less abundant parent, about half of those positions are shared with the more abundant parent. Is it possible that your assembly is what ended up contributing most of the genomic content used in the assembly process, and this is why there is relatively less divergence in your sample relative to the reference?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:21,usability,close,close,21,"@madhubioinfo I will close this issue for now, but you are welcome to reopen it if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/494:218,interoperability,specif,specific-variant-in-my-data,218,"Hi @gevin-MB , . can you take a look at the ""Missing variants where a candidate is generated:"" section in [this FAQ question](https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data), and see if that answers your question? If not, feel free to follow up here again. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:190,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,190,"Hi @gevin-MB , . can you take a look at the ""Missing variants where a candidate is generated:"" section in [this FAQ question](https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data), and see if that answers your question? If not, feel free to follow up here again. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:456,usability,help,help,456,"I found more cofusing genotypes:. ./.:136:116,20:5:0,3,28:II (0/1?). ./.:137:.:.:0,0,0,0,0,0:UU (DP=137,but the ref reads and alt reads is .). ./.:137:.,0:8:0,0,0:11 (DP=137,but the ref readsis .). ./.:137:.:.:0,0,0:OO . ./.:137:2,129:5:0,13,4:II (1/1?). ./.:137:86,50:6:0,4,39:II (0/1?). ./.:137:99,38:6:0,4,31:II (0/1?). ./.:13:10,3:4:0,1,26:II. ./.:141:9,132:3:0,12,0:II (1/1?). How can I deal with the situation(RNC= II OO UU...)? Many thanks for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:419,deployability,log,logic,419,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:277,energy efficiency,predict,prediction,277,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:388,energy efficiency,predict,prediction,388,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:134,interoperability,specif,specifically,134,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:277,safety,predict,prediction,277,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:388,safety,predict,prediction,388,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:419,safety,log,logic,419,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:419,security,log,logic,419,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:419,testability,log,logic,419,"Hi @Suke-fudan , . to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:. ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:. https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1992,integrability,transform,transform,1992,"'t DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this gen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2484,integrability,transform,transform,2484,"r ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1054,interoperability,FORMAT,FORMAT,1054,"https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1992,interoperability,transform,transform,1992,"'t DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this gen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2484,interoperability,transform,transform,2484,"r ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2097,modifiability,deco,decoration-thickness,2097,"eason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2133,modifiability,deco,decoration-style,2133,"M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2165,modifiability,deco,decoration-color,2165,"ta, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2589,modifiability,deco,decoration-thickness,2589,"' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2625,modifiability,deco,decoration-style,2625,"population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2657,modifiability,deco,decoration-color,2657,"VCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:214,performance,network,network,214,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:992,reliability,doe,doesn,992,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3709,reliability,doe,does,3709,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:878,safety,input,input,878,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1186,safety,input,input,1186,"it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1683,safety,input,input,1683,", low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2961,safety,input,input,2961,"art; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3030,safety,input,input,3030,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3487,safety,safe,safely,3487,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3692,safety,safe,safely,3692,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:214,security,network,network,214,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:553,testability,coverag,coverage,553,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:731,testability,coverag,coverage,731,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1238,testability,coverag,coverage,1238,"c region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1456,testability,assert,assertion,1456,"and-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3923,testability,assert,assertion,3923,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:276,usability,learn,learned,276,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:284,usability,experien,experience,284,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:878,usability,input,input,878,"Thank you! You are right.The ""RNC"" comes from GLnexus. . https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1186,usability,input,input,1186,"it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage."". So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1683,usability,input,input,1683,", low base quality in reads, and overall low coverage)? When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1. However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II. In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)? ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">. https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs. One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. . <html>. <body>. <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:2961,usability,input,input,2961,"art; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:3030,usability,input,input,3030,"te-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation. -- | -- | --. . | N/A | corresponding allele is called. M | Missing data | input (gVCF) had no data at this genome position. P | Partial data | input only partially covered this genome position. D | Depth | read depth too low to call. – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s). L | Lost allele | ^ but other than deletion allele. U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s). O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s). 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->. </body>. </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:992,deployability,pipelin,pipeline,992,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:461,integrability,topic,topic,461,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:992,integrability,pipelin,pipeline,992,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:234,safety,input,input,234,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:240,security,sign,signals,240,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:566,security,expos,exposed,566,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:234,usability,input,input,234,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:1102,usability,help,helps,1102,"Hi @Suke-fudan ,. Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/495:106,deployability,updat,update,106,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:380,deployability,contain,contains,380,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:393,deployability,log,logic,393,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:740,deployability,updat,update,740,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:537,energy efficiency,model,model,537,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:690,energy efficiency,model,modeling,690,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:54,integrability,sub,substantial,54,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:512,integrability,sub,subclass,512,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:563,modifiability,layer,layer,563,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:106,safety,updat,update,106,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:393,safety,log,logic,393,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:740,safety,updat,update,740,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:106,security,updat,update,106,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:393,security,log,logic,393,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:434,security,modif,modify,434,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:537,security,model,model,537,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:546,security,modif,modify,546,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:690,security,model,modeling,690,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:740,security,updat,update,740,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:222,testability,context,context,222,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:393,testability,log,logic,393,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do? [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:331,energy efficiency,model,model,331,"For the part of changing the output classes, we have a parameter in make_examples that enables that without code changes: look for ""customized_classes"" in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. However, I don't think we have a set way to enable retraining just the final layer of the model, although @pichuan might know more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:55,modifiability,paramet,parameter,55,"For the part of changing the output classes, we have a parameter in make_examples that enables that without code changes: look for ""customized_classes"" in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. However, I don't think we have a set way to enable retraining just the final layer of the model, although @pichuan might know more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:318,modifiability,layer,layer,318,"For the part of changing the output classes, we have a parameter in make_examples that enables that without code changes: look for ""customized_classes"" in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. However, I don't think we have a set way to enable retraining just the final layer of the model, although @pichuan might know more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:331,security,model,model,331,"For the part of changing the output classes, we have a parameter in make_examples that enables that without code changes: look for ""customized_classes"" in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. However, I don't think we have a set way to enable retraining just the final layer of the model, although @pichuan might know more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:504,availability,sli,slim,504,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:141,energy efficiency,current,currently,141,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:330,energy efficiency,current,currently,330,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:382,energy efficiency,current,currently,382,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:426,modifiability,layer,layer,426,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:374,reliability,doe,doesn,374,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:504,reliability,sli,slim,504,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:65,security,hardcod,hardcoded,65,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:541,security,modif,modify,541,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:273,usability,support,support,273,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:392,usability,support,support,392,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:607,usability,support,support,607,"Hi @ed5152 ,. Thanks for your question. Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes. In the future we might support more flexible number of classes, but this is not currently on our roadmap. Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/496:290,energy efficiency,model,model,290,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:452,energy efficiency,model,model,452,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:1210,interoperability,share,share,1210,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:290,security,model,model,290,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:452,security,model,model,452,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:563,security,sign,significantly,563,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:281,usability,learn,learning,281,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:557,usability,help,helps,557,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:725,usability,support,support,725,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:1148,usability,user,users,1148,"Hi! DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing. There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:230,deployability,contain,contains,230,"Hello Maria,. Thanks for your reply, this is very clear. I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:293,integrability,sub,subset,293,"Hello Maria,. Thanks for your reply, this is very clear. I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:263,modifiability,Pac,PacBio,263,"Hello Maria,. Thanks for your reply, this is very clear. I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:50,usability,clear,clear,50,"Hello Maria,. Thanks for your reply, this is very clear. I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/497:18,availability,error,error,18,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:24,integrability,messag,message,24,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:8,interoperability,share,share,8,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:24,interoperability,messag,message,24,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:18,performance,error,error,18,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:18,safety,error,error,18,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:18,usability,error,error,18,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:88,usability,help,help,88,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,availability,error,error,66,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:72,integrability,messag,messages,72,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:72,interoperability,messag,messages,72,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,performance,error,error,66,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,safety,error,error,66,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,usability,error,error,66,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:146,usability,user,user-images,146,"Thanks for getting back to me. I have attached the screenshot for error messages:. <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:147,energy efficiency,model,model,147,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,integrability,repositor,repository,66,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:66,interoperability,repositor,repository,66,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:176,reliability,doe,does,176,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:147,security,model,model,147,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:185,usability,support,support,185,"@myonaung , can you please open this issue in the PEPPER's github repository (https://github.com/kishwarshafin/pepper). I think this issue is with model quantization, your HPC does not support quantized inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:57,integrability,repositor,repository,57,"@kishwarshafin , Thanks. I've opened the issue at Gihtub repository.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:57,interoperability,repositor,repository,57,"@kishwarshafin , Thanks. I've opened the issue at Gihtub repository.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:26,availability,error,error,26,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:61,availability,error,error,61,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:701,availability,mask,mask,701,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:956,availability,mask,mask,956,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1501,availability,operat,operating,1501,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2332,availability,avail,available,2332,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1160,deployability,contain,contained,1160,"w things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1399,deployability,contain,contained,1399,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1669,deployability,version,version,1669,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1813,deployability,releas,release,1813,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:129,energy efficiency,CPU,CPU,129,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:631,energy efficiency,cpu,cpuset,631,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:717,energy efficiency,CPU,CPUs,717,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:744,energy efficiency,cpu,cpuset,744,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:795,energy efficiency,cpu,cpuset,795,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:847,energy efficiency,cpu,cpuset,847,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:857,energy efficiency,cpu,cpuset,857,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1031,energy efficiency,cpu,cpuset,1031,"uld be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1041,energy efficiency,cpu,cpuset,1041,"ething else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1234,energy efficiency,cpu,cpuset,1234,"ty.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1253,energy efficiency,CPU,CPU,1253,"pile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1411,energy efficiency,CPU,CPU,1411,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1418,energy efficiency,CPU,CPU,1418,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1425,energy efficiency,CPU,CPU,1425,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1432,energy efficiency,CPU,CPU,1432,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1439,energy efficiency,CPU,CPU,1439,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1446,energy efficiency,CPU,CPU,1446,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1453,energy efficiency,CPU,CPU,1453,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1460,energy efficiency,CPU,CPU,1460,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1522,energy efficiency,cpu,cpu,1522,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1658,energy efficiency,CPU,CPU,1658,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2296,energy efficiency,CPU,CPU,2296,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2318,energy efficiency,core,cores,2318,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1669,integrability,version,version,1669,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:120,interoperability,specif,specific,120,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1669,modifiability,version,version,1669,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:26,performance,error,error,26,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:61,performance,error,error,61,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:129,performance,CPU,CPU,129,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:631,performance,cpu,cpuset,631,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:717,performance,CPU,CPUs,717,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:744,performance,cpu,cpuset,744,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:795,performance,cpu,cpuset,795,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:847,performance,cpu,cpuset,847,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:857,performance,cpu,cpuset,857,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1031,performance,cpu,cpuset,1031,"uld be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1041,performance,cpu,cpuset,1041,"ething else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1234,performance,cpu,cpuset,1234,"ty.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1253,performance,CPU,CPU,1253,"pile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1411,performance,CPU,CPU,1411,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1418,performance,CPU,CPU,1418,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1425,performance,CPU,CPU,1425,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1432,performance,CPU,CPU,1432,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1439,performance,CPU,CPU,1439,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1446,performance,CPU,CPU,1446,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1453,performance,CPU,CPU,1453,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1460,performance,CPU,CPU,1460,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1522,performance,cpu,cpu,1522,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1658,performance,CPU,CPU,1658,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2296,performance,CPU,CPU,2296,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2332,reliability,availab,available,2332,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:26,safety,error,error,26,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:61,safety,error,error,61,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2332,safety,avail,available,2332,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2332,security,availab,available,2332,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:26,usability,error,error,26,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:61,usability,error,error,61,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:76,usability,indicat,indicate,76,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1583,usability,command,commands,1583,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1722,usability,Command,Command,1722,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1769,usability,Command,Command,1769,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1789,usability,Command,Command,1789,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1824,usability,Command,Command,1824,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2207,usability,command,command,2207,"t you see:. ```C. #define _GNU_SOURCE. #include <pthread.h>. #include <stdio.h>. #include <stdlib.h>. #include <errno.h>. #define handle_error_en(en, msg) \. do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int. main(int argc, char *argv[]). {. int s;. cpu_set_t cpuset;. pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);. for (int j = 0; j < 8; j++). CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);. if (s != 0). handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");. for (int j = 0; j < CPU_SETSIZE; j++). if (CPU_ISSET(j, &cpuset)). printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);. }. ```. You should see something like this:. ```Bash. $ ./affinity. Set returned by pthread_getaffinity_np() contained:. CPU 0. CPU 1. CPU 2. CPU 3. CPU 4. CPU 5. CPU 6. CPU 7. $. ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`. Command 2: `lscpu`. Command 3: `cat /etc/os-release` . Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:101,availability,error,error,101,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,availability,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:816,deployability,api,apic,816,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,deployability,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1458,deployability,releas,release,1458,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1507,deployability,VERSION,VERSION,1507,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2015,deployability,Version,Version,2015,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2288,deployability,releas,release,2288,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2309,deployability,Releas,Release,2309,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:254,energy efficiency,CPU,CPU,254,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:313,energy efficiency,CPU,CPU,313,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:333,energy efficiency,CPU,CPU,333,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:366,energy efficiency,core,core,366,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:375,energy efficiency,Core,Core,375,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:455,energy efficiency,CPU,CPU,455,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:470,energy efficiency,Model,Model,470,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:481,energy efficiency,Model,Model,481,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:510,energy efficiency,CPU,CPU,510,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:549,energy efficiency,CPU,CPU,549,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:568,energy efficiency,CPU,CPU,568,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:592,energy efficiency,CPU,CPU,592,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:734,energy efficiency,CPU,CPU,734,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:759,energy efficiency,CPU,CPU,759,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,energy efficiency,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2025,energy efficiency,core,core-,2025,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2040,energy efficiency,core,core-,2040,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:816,integrability,api,apic,816,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1507,integrability,VERSION,VERSION,1507,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2015,integrability,Version,Version,2015,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:232,interoperability,Architectur,Architecture,232,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:387,interoperability,socket,socket,387,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:399,interoperability,Socket,Socket,399,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:816,interoperability,api,apic,816,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2203,interoperability,Distribut,Distributor,2203,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1507,modifiability,VERSION,VERSION,1507,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2015,modifiability,Version,Version,2015,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:101,performance,error,error,101,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:254,performance,CPU,CPU,254,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:313,performance,CPU,CPU,313,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:333,performance,CPU,CPU,333,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:455,performance,CPU,CPU,455,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:510,performance,CPU,CPU,510,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:549,performance,CPU,CPU,549,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:568,performance,CPU,CPU,568,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:592,performance,CPU,CPU,592,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:661,performance,cach,cache,661,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:677,performance,cach,cache,677,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:692,performance,cach,cache,692,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:708,performance,cach,cache,708,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:734,performance,CPU,CPU,734,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:759,performance,CPU,CPU,759,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:920,reliability,rdt,rdtscp,920,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,reliability,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:101,safety,error,error,101,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,safety,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:387,security,soc,socket,387,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:399,security,Soc,Socket,399,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:470,security,Model,Model,470,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:481,security,Model,Model,481,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1042,testability,monitor,monitor,1042," troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:37,usability,help,help,37,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:101,usability,error,error,101,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```. ./affinity. pthread_setaffinity_np: Invalid argument. ```. ```. 2. cat /proc/sys/kernel/threads-max. 2061146. lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 32. On-line CPU(s) list: 0-31. Thread(s) per core: 1. Core(s) per socket: 16. Socket(s): 2. NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2372,usability,interact,interactive,2372,". NUMA node(s): 2. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz. Stepping: 2. CPU MHz: 2798.211. CPU max MHz: 3600.0000. CPU min MHz: 1200.0000. BogoMIPS: 4600.13. Virtualization: VT-x. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 40960K. NUMA node0 CPU(s): 0-15. NUMA node1 CPU(s): 16-31. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release. NAME=""Red Hat Enterprise Linux Server"". VERSION=""7.9 (Maipo)"". ID=""rhel"". ID_LIKE=""fedora"". VARIANT=""Server"". VARIANT_ID=""server"". VERSION_ID=""7.9"". PRETTY_NAME=""Red Hat Enterprise Linux"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server"". HOME_URL=""https://www.redhat.com/"". BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7"". REDHAT_BUGZILLA_PRODUCT_VERSION=7.9. REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"". REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a. LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch. Distributor ID: RedHatEnterpriseServer. Description: Red Hat Enterprise Linux Server release 7.9 (Maipo). Release: 7.9. Codename: Maipo. ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:189,energy efficiency,cpu,cpu,189,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:283,energy efficiency,cpu,cpu,283,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:294,energy efficiency,cpu,cpuset,294,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1579,energy efficiency,cpu,cpuset,1579,"grosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1666,energy efficiency,cpu,cpuset,1666,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1697,energy efficiency,cpu,cpuset,1697,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1742,energy efficiency,cpu,cpuset,1742,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1787,energy efficiency,cpu,cpuset,1787,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1835,energy efficiency,cpu,cpuset,1835,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1862,energy efficiency,cpu,cpuset,1862,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1883,energy efficiency,cpu,cpuset,1883,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1896,energy efficiency,cpu,cpuset,1896,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1903,energy efficiency,cpu,cpus,1903,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1908,energy efficiency,cpu,cpuset,1908,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1935,energy efficiency,cpu,cpuset,1935,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1957,energy efficiency,cpu,cpuset,1957,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1990,energy efficiency,cpu,cpuset,1990,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2031,energy efficiency,cpu,cpuset,2031,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2067,energy efficiency,cpu,cpuset,2067,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2094,energy efficiency,cpu,cpuset,2094,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2201,energy efficiency,cpu,cpuset,2201,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2208,energy efficiency,cpu,cpuset,2208,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2215,energy efficiency,cpu,cpus,2215,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2246,energy efficiency,cpu,cpuset,2246,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2304,energy efficiency,cpu,cpuset,2304,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2409,energy efficiency,cpu,cpuset,2409,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2416,energy efficiency,cpu,cpuset,2416,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2423,energy efficiency,cpu,cpus,2423,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2465,energy efficiency,cpu,cpuset,2465,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2550,energy efficiency,cpu,cpuset,2550,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:189,performance,cpu,cpu,189,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:283,performance,cpu,cpu,283,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:294,performance,cpu,cpuset,294,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1579,performance,cpu,cpuset,1579,"grosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1666,performance,cpu,cpuset,1666,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1697,performance,cpu,cpuset,1697,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1742,performance,cpu,cpuset,1742,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1787,performance,cpu,cpuset,1787,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1835,performance,cpu,cpuset,1835,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1862,performance,cpu,cpuset,1862,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1883,performance,cpu,cpuset,1883,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1896,performance,cpu,cpuset,1896,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1903,performance,cpu,cpus,1903,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1908,performance,cpu,cpuset,1908,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1935,performance,cpu,cpuset,1935,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1957,performance,cpu,cpuset,1957,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1990,performance,cpu,cpuset,1990,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2031,performance,cpu,cpuset,2031,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2067,performance,cpu,cpuset,2067,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2094,performance,cpu,cpuset,2094,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2201,performance,cpu,cpuset,2201,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2208,performance,cpu,cpuset,2208,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2215,performance,cpu,cpus,2215,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2246,performance,cpu,cpuset,2246,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2304,performance,cpu,cpuset,2304,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2409,performance,cpu,cpuset,2409,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2416,performance,cpu,cpuset,2416,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2423,performance,cpu,cpus,2423,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2465,performance,cpu,cpuset,2465,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2550,performance,cpu,cpuset,2550,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:143,usability,command,commands,143,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:388,usability,USER,USERNAME,388,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:484,usability,USER,USERNAME,484,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:853,usability,statu,status,853,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:932,usability,command,command,932,"@myonaung This tells me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.eff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1014,usability,statu,status,1014,"lls me you have the hardware to do it, now let's see if the OS has the capability turned on. Can you please run the following commands:. 1) ```cat /proc/filesystems | grep cpu```. You should see something like this:. ```Bash. -bash-4.2$ cat /proc/filesystems | grep cpu. nodev cpuset. -bash-4.2$. ```. 2) Next let's see if you have these enabled on a process, replacing `USERNAME` with the one for you and the process we use to check for is `bash`:. ```ps aux | grep USERNAME | grep bash```. For example, for me it returns this:. ```Bash. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notif",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:1548,usability,command,command,1548,"sh. -bash-4.2$ ps aux | grep pgrosu | grep bash. pgrosu 121490 0.0 0.0 127844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:2168,usability,command,commands,2168,"844 3856 pts/3 Ss 18:25 0:00 -bash. pgrosu 121703 0.0 0.0 112812 976 pts/3 S+ 18:40 0:00 grep --color=auto bash. -bash-4.2$. ```. Once you have that, then type, replacing `BASH_PID` with the one for you:. ```cat /proc/BASH_PID/status | grep allowed```. For me, the BASH_PID is 121490, and when you run the command you should see something like this:. ```Bash. -bash-4.2$ cat /proc/121490/status | grep allowed. Cpus_allowed: ffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff. Cpus_allowed_list: 0-271. Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003. Mems_allowed_list: 0-1. -bash-4.2$. ```. 3) Let's see if you have cgroups, by typing this command:. ```ls /sys/fs/cgroup/cpuset/```. You should see something like this:. ```Bash. -bash-4.2$ ls /sys/fs/cgroup/cpuset/. cgroup.clone_children cpuset.memory_pressure. cgroup.event_control cpuset.memory_pressure_enabled. cgroup.procs cpuset.memory_spread_page. cgroup.sane_behavior cpuset.memory_spread_slab. cpuset.cpu_exclusive cpuset.mems. cpuset.cpus cpuset.sched_load_balance. cpuset.effective_cpus cpuset.sched_relax_domain_level. cpuset.effective_mems notify_on_release. cpuset.mem_exclusive release_agent. cpuset.mem_hardwall tasks. cpuset.memory_migrate. -bash-4.2$. ```. If you do, then try the following commands:. ```cat /sys/fs/cgroup/cpuset/cpuset.cpus```. ```cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7```. ```cat /sys/fs/cgroup/cpuset/tasks | head -n 7```. You should see something like this:. ```Bash. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cpuset.cpus. 0-47. -bash-4.2$ cat /sys/fs/cgroup/cpuset/cgroup.procs | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$ cat /sys/fs/cgroup/cpuset/tasks | head -n 7. 1. 2. 6. 7. 8. 9. 10. -bash-4.2$. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:103,deployability,patch,patched,103,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:130,deployability,version,version,130,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:130,integrability,version,version,130,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:130,modifiability,version,version,130,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:103,safety,patch,patched,103,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:103,security,patch,patched,103,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:33,usability,help,help,33,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:250,usability,close,close,250,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/498:344,deployability,Depend,Depending,344,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:344,integrability,Depend,Depending,344,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:174,interoperability,specif,specific-variant-in-my-data,174,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:344,modifiability,Depend,Depending,344,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:146,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,146,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:344,safety,Depend,Depending,344,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:344,testability,Depend,Depending,344,"Hi @zhoudreames ,. Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/499:307,availability,cluster,cluster,307,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:195,deployability,instal,installed,195,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:307,deployability,cluster,cluster,307,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:461,deployability,instal,installed,461,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:562,interoperability,specif,specifically,562,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:471,modifiability,pac,packages,471,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:281,security,access,access,281,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:321,deployability,version,version,321,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:329,deployability,instal,installed,329,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:375,deployability,version,version,375,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:396,deployability,version,version,396,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:321,integrability,version,version,321,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:375,integrability,version,version,375,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:396,integrability,version,version,396,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:321,modifiability,version,version,321,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:375,modifiability,version,version,375,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:396,modifiability,version,version,396,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:383,security,access,access,383,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7? Is it possible for you to check (on both machines) what's the different between the protobuff version installed? (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:15,deployability,instal,installed,15,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:58,deployability,instal,installed,58,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:166,deployability,contain,container,166,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:62,deployability,instal,installed,62,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:204,deployability,instal,installed,204,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:235,deployability,instal,install,235,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:302,deployability,instal,install,302,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:318,deployability,version,version,318,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:451,deployability,instal,installs,451,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:473,deployability,modul,module,473,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:568,deployability,contain,container,568,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:701,deployability,contain,container,701,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:830,deployability,contain,container,830,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:908,deployability,modul,modules,908,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:900,energy efficiency,load,loading,900,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:318,integrability,version,version,318,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:384,integrability,protocol,protocolbuffers,384,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:384,interoperability,protocol,protocolbuffers,384,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:678,interoperability,bind,binding,678,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:721,interoperability,standard,standard,721,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:318,modifiability,version,version,318,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:473,modifiability,modul,module,473,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:515,modifiability,pac,packages,515,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:678,modifiability,bind,binding,678,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:908,modifiability,modul,modules,908,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:900,performance,load,loading,900,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:473,safety,modul,module,473,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:641,safety,prevent,prevent,641,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:908,safety,modul,modules,908,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:641,security,preven,prevent,641,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:443,usability,command,command,443,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:756,usability,behavi,behavior,756,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python? Cheers,. Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:26,interoperability,specif,specifically,26,"I see. Yeah, DeepVariant (specifically Nucleus) has a very specific way it uses protobuf. This is something we hope to improve for the long run. But for now, I don't have a very good general solution here. If you have any workaround for this for now, please just go ahead with it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:59,interoperability,specif,specific,59,"I see. Yeah, DeepVariant (specifically Nucleus) has a very specific way it uses protobuf. This is something we hope to improve for the long run. But for now, I don't have a very good general solution here. If you have any workaround for this for now, please just go ahead with it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:420,deployability,instal,install,420,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:519,deployability,INSTAL,INSTALL,519,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:537,deployability,instal,installation,537,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:565,deployability,version,version,565,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:596,deployability,version,version,596,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:620,deployability,version,version,620,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:175,energy efficiency,cloud,cloud-platform,175,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:247,energy efficiency,cloud,cloud,247,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:565,integrability,version,version,565,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:596,integrability,version,version,596,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:620,integrability,version,version,620,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:181,interoperability,platform,platform,181,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:276,interoperability,standard,standard-,276,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1515,interoperability,specif,specific,1515,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1581,interoperability,share,share,1581,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:565,modifiability,version,version,565,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:596,modifiability,version,version,596,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:620,modifiability,version,version,620,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:318,security,ssh,ssh,318,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:360,security,ssh,ssh,360,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1099,testability,unit,unittest,1099,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:123,usability,USER,USER,123,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:366,usability,USER,USER,366,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1502,usability,support,support,1502,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". ```. ssh into the machine. ```. gcloud compute ssh ${USER}-centos7 --zone us-west1-b. ```. On the machine, install singularity:. I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```. $ singularity --version. singularity-ce version 3.9.2. ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/500:539,energy efficiency,model,model,539,"My apologies, I guess I needed to edit my command for the make_examples. /opt/deepvariant/bin/make_examples \. --mode training \. --ref reference.fasta \. --reads 3-to-reference.bam \. --examples examples/training_set.with_label.tfrecord.gz \. --confident_regions reference.bed \. --truth_variants 3-asm.paf.vcf.gz \. --sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true --alt_aligned_pileup=diff_channels. Such that I now included the `bottom line` of the above command (my BAMs do not need to be phased for this model for haploid organisms I guess?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:539,security,model,model,539,"My apologies, I guess I needed to edit my command for the make_examples. /opt/deepvariant/bin/make_examples \. --mode training \. --ref reference.fasta \. --reads 3-to-reference.bam \. --examples examples/training_set.with_label.tfrecord.gz \. --confident_regions reference.bed \. --truth_variants 3-asm.paf.vcf.gz \. --sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true --alt_aligned_pileup=diff_channels. Such that I now included the `bottom line` of the above command (my BAMs do not need to be phased for this model for haploid organisms I guess?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:42,usability,command,command,42,"My apologies, I guess I needed to edit my command for the make_examples. /opt/deepvariant/bin/make_examples \. --mode training \. --ref reference.fasta \. --reads 3-to-reference.bam \. --examples examples/training_set.with_label.tfrecord.gz \. --confident_regions reference.bed \. --truth_variants 3-asm.paf.vcf.gz \. --sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true --alt_aligned_pileup=diff_channels. Such that I now included the `bottom line` of the above command (my BAMs do not need to be phased for this model for haploid organisms I guess?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:488,usability,command,command,488,"My apologies, I guess I needed to edit my command for the make_examples. /opt/deepvariant/bin/make_examples \. --mode training \. --ref reference.fasta \. --reads 3-to-reference.bam \. --examples examples/training_set.with_label.tfrecord.gz \. --confident_regions reference.bed \. --truth_variants 3-asm.paf.vcf.gz \. --sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true --alt_aligned_pileup=diff_channels. Such that I now included the `bottom line` of the above command (my BAMs do not need to be phased for this model for haploid organisms I guess?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/501:10,integrability,discover,discovered,10,"I've just discovered that when I change glnexus config from `DeepVariantWES` to `DeepVariant` (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity) which seems more probable in relation to 1kg 1.7 mln from 3500 samples. Anyway, would be nice to hear if it sounds reasonable for exome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:10,interoperability,discover,discovered,10,"I've just discovered that when I change glnexus config from `DeepVariantWES` to `DeepVariant` (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity) which seems more probable in relation to 1kg 1.7 mln from 3500 samples. Anyway, would be nice to hear if it sounds reasonable for exome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:10,usability,discov,discovered,10,"I've just discovered that when I change glnexus config from `DeepVariantWES` to `DeepVariant` (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity) which seems more probable in relation to 1kg 1.7 mln from 3500 samples. Anyway, would be nice to hear if it sounds reasonable for exome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:115,usability,help,help,115,"Hi @kriestof . Sorry for the delay of response, we are off work until tomorrow so we haven't responded. If you can help with a few clarifications that will be great:. 1. I don't expect the variants in the VCF from DeepVariant to be 10x less. Are you seeing 10x less variants even just from the VCF files? 2. You later mentioned ""when I change glnexus config from DeepVariantWES to DeepVariant (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity)"" -- how many calls did you get from 75 samples when you used ""DeepVariantWES""? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:249,performance,time,time,249,"Hello @pichuan ,. Thank you for your response! Right now I get the right bed file from my data provider. It seems far better: ~320k calls in a single file, while ~420k after joint calling with glnexus and DeepVariantWES config. I remember the first time I got this data Deepvariant has much less calls than GATK, but then calling was made by our data provider and I don't possess any details on that. I can't remind the exact number, but I think it was far below 300k calls. Most of my comparison was based on that *legacy* data. Sorry for that unfair and unchecked comparison. In case you are interested I could try to generate a fresh GATK calling with the same bed file according to the GATK best practices. Otherwise I could let you know if I give a try to GATK in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:700,reliability,pra,practices,700,"Hello @pichuan ,. Thank you for your response! Right now I get the right bed file from my data provider. It seems far better: ~320k calls in a single file, while ~420k after joint calling with glnexus and DeepVariantWES config. I remember the first time I got this data Deepvariant has much less calls than GATK, but then calling was made by our data provider and I don't possess any details on that. I can't remind the exact number, but I think it was far below 300k calls. Most of my comparison was based on that *legacy* data. Sorry for that unfair and unchecked comparison. In case you are interested I could try to generate a fresh GATK calling with the same bed file according to the GATK best practices. Otherwise I could let you know if I give a try to GATK in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:230,safety,reme,remember,230,"Hello @pichuan ,. Thank you for your response! Right now I get the right bed file from my data provider. It seems far better: ~320k calls in a single file, while ~420k after joint calling with glnexus and DeepVariantWES config. I remember the first time I got this data Deepvariant has much less calls than GATK, but then calling was made by our data provider and I don't possess any details on that. I can't remind the exact number, but I think it was far below 300k calls. Most of my comparison was based on that *legacy* data. Sorry for that unfair and unchecked comparison. In case you are interested I could try to generate a fresh GATK calling with the same bed file according to the GATK best practices. Otherwise I could let you know if I give a try to GATK in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/502:494,integrability,pub,publicly,494,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md. You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0. You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:226,security,access,accessible,226,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md. You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0. You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:383,usability,document,documentation,383,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md. You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0. You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:552,usability,help,help,552,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md. You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0. You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:609,usability,document,documented,609,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md. You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0. You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:59,usability,help,helped,59,"Hi Maria, . Thank you so much for your response, it really helped! Best wishes,. Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/503:248,availability,Sli,Slide,248,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2416,availability,Sli,Slide,2416,"pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	backgroun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2254,deployability,fail,failed,2254,"5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-righ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:237,energy efficiency,Power,PowerPoint,237,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:296,energy efficiency,Power,PowerPoint,296,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2405,energy efficiency,Power,PowerPoint,2405,"right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2464,energy efficiency,Power,PowerPoint,2464,"nd:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:117,integrability,schema,schemas,117,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2285,integrability,schema,schemas,2285,"text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:101,interoperability,xml,xmlns,101,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:161,interoperability,xml,xmlns,161,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2269,interoperability,xml,xmlns,2269,"ern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2329,interoperability,xml,xmlns,2329,". 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:569,modifiability,deco,decoration,569,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2737,modifiability,deco,decoration,2737,"er;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:229,performance,content,content,229,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:277,performance,content,content,277,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2397,performance,content,content,2397,"	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2445,performance,content,content,2445,"id white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:248,reliability,Sli,Slide,248,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2254,reliability,fail,failed,2254,"5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-righ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2416,reliability,Sli,Slide,2416,"pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	backgroun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2243,safety,valid,validation,2243,"ound:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	pad",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:2243,security,validat,validation,2243,"ound:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 | 43,0,50. 38 | 38 | 91,0.41 | 38 | 38,0,50. 48 | 48 | 136,0.51 | 48 | 48,0,62. 43 | 43 | 31,0.42 | 19 | 19,0,46. 43 | 43 | 33,0.45 | 41 | 41,0,55. 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->. </body>. </html>. The following mutation validation failed:. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	pad",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:71,testability,verif,verified,71,"Hi!I did some PCR and Sanger sequencing. The following mutation can be verified successfully：. <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml"". xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>. <meta name=Generator content=""Microsoft PowerPoint 15"">. <style>. <!--tr. 	{mso-height-source:auto;}. col. 	{mso-width-source:auto;}. td. 	{padding-top:1.0px;. 	padding-right:1.0px;. 	padding-left:1.0px;. 	mso-ignore:padding;. 	color:windowtext;. 	font-size:18.0pt;. 	font-weight:400;. 	font-style:normal;. 	text-decoration:none;. 	font-family:Arial;. 	mso-generic-font-family:auto;. 	mso-font-charset:0;. 	text-align:general;. 	vertical-align:bottom;. 	border:none;. 	mso-background-source:auto;. 	mso-pattern:auto;}. .oa1. 	{border-top:1.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:3.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#4472C4;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa2. 	{border-top:3.0pt solid white;. 	border-right:1.0pt solid white;. 	border-bottom:1.0pt solid white;. 	border-left:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa3. 	{border:1.0pt solid white;. 	background:#E9EBF5;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. .oa4. 	{border:1.0pt solid white;. 	background:#CFD5EA;. 	mso-pattern:auto none;. 	text-align:center;. 	vertical-align:middle;. 	padding-bottom:3.6pt;. 	padding-left:7.2pt;. 	padding-top:3.6pt;. 	padding-right:7.2pt;}. -->. </style>. </head>. <body>. <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL. -- | -- | -- | -- | --. 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:187,availability,error,error,187,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,availability,error,error,265,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:656,availability,error,error,656,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:299,deployability,scale,scale,299,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:299,energy efficiency,scale,scale,299,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:632,energy efficiency,estimat,estimated,632,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:35,integrability,filter,filter,35,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:299,modifiability,scal,scale,299,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:187,performance,error,error,187,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,performance,error,error,265,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:299,performance,scale,scale,299,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:656,performance,error,error,656,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:187,safety,error,error,187,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,safety,error,error,265,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:656,safety,error,error,656,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:187,usability,error,error,187,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,usability,error,error,265,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:338,usability,indicat,indicates,338,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:404,usability,indicat,indicates,404,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:656,usability,error,error,656,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:671,usability,help,help,671,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/505:497,interoperability,coordinat,coordinate,497,"Hi @chregu1971 . Are the calls missing from the end of the reference genome itself (meaning, these are at the boundary of the contig that yuo provided)? Or are they at the end of the amplicon, but still within the reference genome? . DeepVariant does not generate pileup windows for sequence that is less than 110 bp from the edge of a reference contig. If this is the case, the only way to get calls here would be to pad the reference sequence (note that at the contig start this would alter the coordinate system of all calls). Can you let me know whether these regions are at the edge of contigs or not? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:246,reliability,doe,does,246,"Hi @chregu1971 . Are the calls missing from the end of the reference genome itself (meaning, these are at the boundary of the contig that yuo provided)? Or are they at the end of the amplicon, but still within the reference genome? . DeepVariant does not generate pileup windows for sequence that is less than 110 bp from the edge of a reference contig. If this is the case, the only way to get calls here would be to pad the reference sequence (note that at the contig start this would alter the coordinate system of all calls). Can you let me know whether these regions are at the edge of contigs or not? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:63,deployability,observ,observation,63,"Hi @AndrewCarroll,. Thank you for the answer. This explains my observation. Did I miss this in some documentation? The missing calls are from the end of the amplicon which corresponds to the end of the reference. It is actually a circular reference. . Duplicating the reference solved the problem, as it effectively works like padding. Best regards,. Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:63,testability,observ,observation,63,"Hi @AndrewCarroll,. Thank you for the answer. This explains my observation. Did I miss this in some documentation? The missing calls are from the end of the amplicon which corresponds to the end of the reference. It is actually a circular reference. . Duplicating the reference solved the problem, as it effectively works like padding. Best regards,. Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:100,usability,document,documentation,100,"Hi @AndrewCarroll,. Thank you for the answer. This explains my observation. Did I miss this in some documentation? The missing calls are from the end of the amplicon which corresponds to the end of the reference. It is actually a circular reference. . Duplicating the reference solved the problem, as it effectively works like padding. Best regards,. Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:304,usability,effectiv,effectively,304,"Hi @AndrewCarroll,. Thank you for the answer. This explains my observation. Did I miss this in some documentation? The missing calls are from the end of the amplicon which corresponds to the end of the reference. It is actually a circular reference. . Duplicating the reference solved the problem, as it effectively works like padding. Best regards,. Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:26,testability,plan,plan,26,Thanks @chregu1971 . I'll plan to add this to the FAQ.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/506:34,deployability,Contain,Container,34,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:554,deployability,contain,container,554,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:130,interoperability,bind,bind,130,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:150,interoperability,bind,bind,150,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:160,interoperability,specif,specification,160,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:188,interoperability,format,format,188,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:339,interoperability,specif,specified,339,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:422,interoperability,bind,bind,422,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:501,interoperability,bind,binds,501,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:130,modifiability,bind,bind,130,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:150,modifiability,bind,bind,150,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:422,modifiability,bind,bind,422,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:501,modifiability,bind,binds,501,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:9,usability,help,help,9,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:69,usability,guid,guides,69,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:80,usability,user,user-guide,80,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:145,usability,user,user-bind,145,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):. ```. -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list. ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/510:212,deployability,automat,automatically,212,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:236,integrability,sub,subset,236,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:195,modifiability,paramet,parameter,195,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:94,reliability,doe,does,94,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:297,reliability,Doe,Does,297,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:212,testability,automat,automatically,212,"You don't have to split up the regions into a bunch of separate bed files because DeepVariant does that on its own. When you run DeepVariant with `run_deepvariant` you can set the `--num_shards` parameter, which automatically assigns a subset of regions to the different shards to make them even. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:102,performance,parallel,parallel,102,"Thank you for the fast reply. Almost, I think. Would it change results though to start deepvariant in parallel with a few interval files? I am thinking of using deepvariant within a nextflow script and using multiple smaller machines in parallel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:237,performance,parallel,parallel,237,"Thank you for the fast reply. Almost, I think. Would it change results though to start deepvariant in parallel with a few interval files? I am thinking of using deepvariant within a nextflow script and using multiple smaller machines in parallel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:215,deployability,stage,stages,215,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:473,deployability,stage,stages,473,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:585,energy efficiency,CPU,CPUs,585,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:641,energy efficiency,GPU,GPU,641,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:585,performance,CPU,CPUs,585,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:641,performance,GPU,GPU,641,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:870,performance,parallel,parallelization,870,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:141,usability,effectiv,effectively,141,"You can definitely do your own sharding by supplying different `--regions` to each machine if that is convenient for you. The results should effectively be the same. Another option I want to call out is calling the stages (make_examples, call_variants, postprocess_variants) separately. One thing you might want to try is running `run_deepvariant` with `--dry_run` and the number of shards you want, and then you can see the steps that are actually run underneath. Running stages separately is mostly only worth it if you are processing many samples and want to take advantage of many CPUs for make_examples and separate instances with more GPU for call_variants. With one sample, it's probably faster to run it on one machine, which takes about 7 hours. Are you trying to run on many samples or some other reason why running on one machine is not fast enough and needs parallelization?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:91,deployability,pipelin,pipeline,91,Thanks a lot. We want to support both to run many patients as well as single ones within a pipeline.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:91,integrability,pipelin,pipeline,91,Thanks a lot. We want to support both to run many patients as well as single ones within a pipeline.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:25,usability,support,support,25,Thanks a lot. We want to support both to run many patients as well as single ones within a pipeline.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/511:5,deployability,updat,updated,5,"I've updated tensorflow to 2.7.0 with `sed -i ""s|2.5.0|2.7.0|"" ""deepvariant/settings.sh""` and everything working fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:5,safety,updat,updated,5,"I've updated tensorflow to 2.7.0 with `sed -i ""s|2.5.0|2.7.0|"" ""deepvariant/settings.sh""` and everything working fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:5,security,updat,updated,5,"I've updated tensorflow to 2.7.0 with `sed -i ""s|2.5.0|2.7.0|"" ""deepvariant/settings.sh""` and everything working fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:82,deployability,releas,release,82,Thanks for letting us know. I've made a note of this to look into it for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:16,deployability,updat,update,16,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:44,deployability,releas,release,44,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:68,deployability,updat,update,68,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:107,deployability,version,version,107,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:182,deployability,instal,install,182,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:253,deployability,updat,update,253,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:266,deployability,version,version,266,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:303,deployability,version,version,303,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:311,deployability,updat,updates,311,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:107,integrability,version,version,107,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:266,integrability,version,version,266,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:303,integrability,version,version,303,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:107,modifiability,version,version,107,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:266,modifiability,version,version,266,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:303,modifiability,version,version,303,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:16,safety,updat,update,16,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:68,safety,updat,update,68,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:253,safety,updat,update,253,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:311,safety,updat,updates,311,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:16,security,updat,update,16,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:68,security,updat,update,68,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:253,security,updat,update,253,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:311,security,updat,updates,311,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0. Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/512:106,usability,close,close,106,"This happens when the variant is within 110 bp of the edge of a reference chromosome, or in your case too close to the edge of a scaffold. DeepVariant is built for calling on alignments to a reference genome, so on assemblies it will still work, but for things like this you may need to be careful. . For reference, this is the relevant code: https://github.com/google/deepvariant/blob/5469fec47e57febba751c30f1eb587cbffea8d89/deepvariant/pileup_image.py#L436 -- it is when this function returns None that it triggers that warning in make_examples.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/513:88,availability,error,error,88,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:136,availability,error,error,136,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:48,integrability,pub,public,48,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:220,integrability,pub,public,220,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:88,performance,error,error,88,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:136,performance,error,error,136,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:150,reliability,doe,doesn,150,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:88,safety,error,error,88,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:136,safety,error,error,136,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:312,safety,sanit,sanity,312,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:417,safety,permiss,permission,417,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:121,security,Authenticat,Authentication,121,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:312,security,sanit,sanity,312,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:88,usability,error,error,88,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:136,usability,error,error,136,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:328,usability,confirm,confirmed,328,"Can you check whether you're able to pull other public images on this machine? From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:. ```. singularity pull docker://google/deepvariant:""1.3.0"". ```. as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:503,availability,error,error,503,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:556,availability,ERROR,ERROR,556,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:57,deployability,version,version,57,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:564,deployability,Fail,Failed,564,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:57,integrability,version,version,57,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:57,modifiability,version,version,57,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:503,performance,error,error,503,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:556,performance,ERROR,ERROR,556,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:564,reliability,Fail,Failed,564,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:297,safety,input,input,297,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:503,safety,error,error,503,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:556,safety,ERROR,ERROR,556,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:545,security,sandbox,sandbox,545,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:171,usability,command,command,171,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:297,usability,input,input,297,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:503,usability,error,error,503,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:513,usability,stop,stop,513,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:556,usability,ERROR,ERROR,556,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:581,usability,user,user,581,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:597,usability,user,user,597,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:616,usability,support,supported,616,Thank you very much for your quick replay. I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command . singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10. it returns the following error and stop. INFO: Convert SIF file to sandbox... ERROR : Failed to create user namespace: user namespace not supported by your system. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/514:32,deployability,contain,contains,32,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:52,deployability,version,version,52,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:70,energy efficiency,GPU,GPU,70,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:52,integrability,version,version,52,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:52,modifiability,version,version,52,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:70,performance,GPU,GPU,70,"Hi Phillip,. DeepVariant docker contains a prebuilt version of TF for GPU. Could you try to run it in Conda without pre-existing TF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:49,availability,error,error,49,Removing the conda environment gives me the same error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:49,performance,error,error,49,Removing the conda environment gives me the same error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:49,safety,error,error,49,Removing the conda environment gives me the same error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:49,usability,error,error,49,Removing the conda environment gives me the same error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:223,availability,error,errors,223,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:84,deployability,releas,release,84,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:131,energy efficiency,GPU,GPU,131,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:180,energy efficiency,GPU,GPU,180,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:131,performance,GPU,GPU,131,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:180,performance,GPU,GPU,180,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:223,performance,error,errors,223,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:223,safety,error,errors,223,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:223,usability,error,errors,223,"@Phillip-a-richmond Thanks for checking. I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:13,energy efficiency,gpu,gpu,13,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:13,performance,gpu,gpu,13,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:70,safety,Test,Testing,70,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:70,testability,Test,Testing,70,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:18,availability,error,error,18,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:274,deployability,modul,module,274,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:416,deployability,modul,module,416,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:728,energy efficiency,core,core,728,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:87,interoperability,platform,platform,87,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:274,modifiability,modul,module,274,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:368,modifiability,pac,packages,368,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:416,modifiability,modul,module,416,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:514,modifiability,pac,packages,514,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:708,modifiability,pac,packages,708,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:18,performance,error,error,18,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:18,safety,error,error,18,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:274,safety,modul,module,274,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:416,safety,modul,module,416,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:177,testability,Trace,Traceback,177,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:18,usability,error,error,18,"1.2 produces same error. ```. 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3900,availability,error,error,3900,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:374,deployability,Instal,Install,374,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:654,deployability,version,version,654,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:706,deployability,version,version,706,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:727,deployability,version,version,727,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2286,deployability,version,version,2286,"ef=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:32,energy efficiency,GPU,GPU,32,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:160,energy efficiency,gpu,gpu-machine-on-google-cloud-platform,160,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:232,energy efficiency,gpu,gpu,232,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:263,energy efficiency,gpu,gpu,263,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:382,energy efficiency,GPU,GPU,382,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:685,energy efficiency,gpu,gpu,685,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1035,energy efficiency,gpu,gpu,1035,"achine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1095,energy efficiency,gpu,gpu,1095,"eepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1219,energy efficiency,gpu,gpu,1219,"an@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1829,energy efficiency,gpu,gpu,1829,"gle/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2317,energy efficiency,gpu,gpu,2317,"hr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2426,energy efficiency,gpu,gpu,2426,"0,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2723,energy efficiency,gpu,gpu,2723,"ularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2832,energy efficiency,gpu,gpu,2832,"\. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you sp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3255,energy efficiency,gpu,gpu,3255,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3364,energy efficiency,gpu,gpu,3364,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3491,energy efficiency,core,core,3491,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3604,energy efficiency,gpu,gpu,3604,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:654,integrability,version,version,654,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:706,integrability,version,version,706,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:727,integrability,version,version,727,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2286,integrability,version,version,2286,"ef=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:188,interoperability,platform,platform,188,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2578,interoperability,platform,platform,2578,"ermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2981,interoperability,platform,platform,2981,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:654,modifiability,version,version,654,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:706,modifiability,version,version,706,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:727,modifiability,version,version,727,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2286,modifiability,version,version,2286,"ef=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3101,modifiability,pac,packages,3101,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3402,modifiability,pac,packages,3402,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3644,modifiability,pac,packages,3644,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3714,modifiability,pac,packages,3714,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:32,performance,GPU,GPU,32,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:160,performance,gpu,gpu-machine-on-google-cloud-platform,160,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:232,performance,gpu,gpu,232,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:263,performance,gpu,gpu,263,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:382,performance,GPU,GPU,382,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:685,performance,gpu,gpu,685,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1035,performance,gpu,gpu,1035,"achine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1095,performance,gpu,gpu,1095,"eepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1219,performance,gpu,gpu,1219,"an@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1829,performance,gpu,gpu,1829,"gle/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2317,performance,gpu,gpu,2317,"hr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2426,performance,gpu,gpu,2426,"0,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2503,performance,cach,cached,2503,"UTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2723,performance,gpu,gpu,2723,"ularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2832,performance,gpu,gpu,2832,"\. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you sp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2906,performance,cach,cached,2906,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3255,performance,gpu,gpu,3255,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3364,performance,gpu,gpu,3364,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3435,performance,cach,cached,3435,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3604,performance,gpu,gpu,3604,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3900,performance,error,error,3900,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:757,safety,test,test,757,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:902,safety,test,test,902,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3900,safety,error,error,3900,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3676,security,access,access,3676,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:757,testability,test,test,757,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:902,testability,test,test,902,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1324,testability,unit,unittest,1324,"C 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1934,testability,unit,unittest,1934," Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:56,usability,command,command,56,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:146,usability,command,command-for-a-gpu-machine-on-google-cloud-platform,146,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:. ```. pichuan@pichuan-gpu:~$ uname -a. Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux. ```. # Install GPU driver and Singularity on the machine:. ```. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash. curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash. ```. Singularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1640,usability,command,command,1640,"ngularity version:. ```. pichuan@pichuan-gpu:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1683,usability,command,command,1683,"u:~$ singularity --version. singularity version 3.7.0. ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```. # Pull the image. BIN_VERSION=1.3.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2252,usability,command,command,2252,"variant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. The command above worked, so I copy/pasted the command from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2683,usability,confirm,confirm,2683,"and from the original post:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3215,usability,command,commands,3215,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3551,usability,tool,tools,3551,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3788,usability,help,helpful,3788,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:3900,usability,error,error,3900,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. which also seems to work. This command below shows my TensorFlow version:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'. INFO: Using cached SIF image. 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. 2.5.0. ```. To confirm the path:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'. INFO: Using cached SIF image. 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py. ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```. pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow. INFO: Using cached SIF image. __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src. ```. ```. pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow. ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory. ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:111,usability,close,close,111,"Hi @Phillip-a-richmond ,. if you have any suggestions on how to reproduce this issue, please let me know. I'll close this for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1221,availability,error,error,1221,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:44,deployability,version,version,44,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1010,deployability,version,versions,1010,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1244,deployability,deploy,deployment,1244,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1384,deployability,version,versions,1384,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1238,energy efficiency,cloud,cloud,1238,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:44,integrability,version,version,44,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1010,integrability,version,versions,1010,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:. ```. WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/. export SINGULARITY_CACHEDIR=$WORKING_DIR. export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/. mkdir -p $WORKING_DIR/tmp/. singularity exec \. 	-e \. 	-c \. 	-H $WORKING_DIR \. 	-B $WORKING_DIR/tmp:/tmp \. 	-B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/genomedir/$FASTA_FILE"" \. --reads=""/bamdir/$PROBAND_BAM"" \. --output_vcf=""/output/$PROBAND_VCF"" \. --output_gvcf=""/output/$PROBAND_GVCF"" \. --intermediate_results_dir=""/output/intermediate"" \. --num_shards=$NSLOTS . ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
