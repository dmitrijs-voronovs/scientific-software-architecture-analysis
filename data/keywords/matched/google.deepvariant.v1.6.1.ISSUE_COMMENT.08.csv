id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/145:2349,testability,log,log,2349,"pt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2377,testability,Test,Testing,2377,"eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2454,testability,Test,Test,2454,"24b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2518,testability,Trace,Traceback,2518,"-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3289,testability,Trace,Traceback,3289,"estlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: /usr/local/cuda-10.0/lib64/libcublas.so.9.0: version `libcublas.so.9.0' not found (required by /root/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. However:. 1. I specified CUDA 10 in `settings.sh`. 1. CUDA 9 is not required for TensorFlow `r1.12`. Additionally:. 1. CUDA 9 is not available for my system, Ubuntu 18. 1. Symlinking to the correct file names as ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4724,testability,log,log,4724,"thon/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: /usr/local/cuda-10.0/lib64/libcublas.so.9.0: version `libcublas.so.9.0' not found (required by /root/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. However:. 1. I specified CUDA 10 in `settings.sh`. 1. CUDA 9 is not required for TensorFlow `r1.12`. Additionally:. 1. CUDA 9 is not available for my system, Ubuntu 18. 1. Symlinking to the correct file names as suggested [elsewhere](https://github.com/tensorflow/tensorflow/issues/15604) did not work. 1. I have built TensorFlow `r1.12` (and master) for CUDA 10 (and 9) in my environment previously. Questions:. 1. Does deepvariant have a requirement for CUDA 9(.0?)? 1. How would you recommend proceeding? _________. ```. Linux localhost 4.15.0-1032-aws #34-Ubuntu SMP Thu Jan 17 15:18:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. ```. Full log:. ```. + source settings.sh. ++ export DV_USE_PREINSTALLED_TF=0. ++ DV_USE_PREINSTALLED_TF=0. ++ export TF_CUDA_CLANG=0. ++ TF_CUDA_CLANG=0. ++ export TF_ENABLE_XLA=1. ++ TF_ENABLE_XLA=1. ++ export TF_NEED_CUDA=1. ++ TF_NEED_CUDA=1. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export TF_NEED_GDR=0. ++ TF_NEED_GDR=0. ++ export TF_NEED_HDFS=0. ++ TF_NEED_HDFS=0. ++ export TF_NEED_JEMALLOC=0. ++ TF_NEED_JEMALLOC=0. ++ export TF_NEED_MKL=1. ++ TF_NEED_MKL=1. ++ export TF_NEED_MPI=0. ++ TF_NEED_MPI=0. ++ export TF_NEED_OPENCL=0. ++ TF_NEED_OPENCL=0. ++ export TF_NEED_OPENCL_SYCL=0. ++ TF_NEED_OPENCL_SYCL=0. ++ export TF_NEED_S3=1. ++ TF_NEED_S3=1. ++ export TF_NEED_VERBS=0. ++ TF_NEED_VERBS=0. ++ export TF_CUDA_VERSION=10.0. ++ TF_CUDA_VERSION=10.0. ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda. ++ CUDA_TOOLKIT_PATH=/usr/local/cuda. ++ export TF_CUDNN_VERSION=7. ++ TF_CUDNN_VERSION=7. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. +",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9715,testability,coverag,coverage,9715,"orflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9739,testability,coverag,coverage,9739,"_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9769,testability,test,test,9769,"t/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10267,testability,depend,dependency,10267,"=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10355,testability,test,test,10355,"=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10390,testability,test,test,10390,"pare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10760,testability,test,test,10760,"cified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11252,testability,test,test,11252,"y Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11519,testability,test,testlogs,11519,"mmand>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11569,testability,test,test,11569,"the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11574,testability,log,log,11574,"VM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11602,testability,Test,Testing,11602,"arget-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11676,testability,Test,Test,11676,"o-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11737,testability,Trace,Traceback,11737," = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:13161,testability,Trace,Traceback,13161,"_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:13996,testability,trace,trace,13996,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14304,testability,test,testlogs,14304,"rom tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14348,testability,test,test,14348,"ternal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14353,testability,log,log,14353,"l import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14381,testability,Test,Testing,14381,"/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14449,testability,Test,Test,14449,"nternal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14504,testability,Trace,Traceback,14504,"rnal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14817,testability,test,testing,14817,"ile or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:15054,testability,test,testing,15054,"========================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:15618,testability,Trace,Traceback,15618,"google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16453,testability,trace,trace,16453,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16624,testability,test,tests,16624," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__ini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16641,testability,Test,Testing,16641," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16889,testability,test,testlogs,16889,"ap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16929,testability,test,test,16929,"n <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-pac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16934,testability,log,log,16934,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16962,testability,Test,Testing,16962,"rnal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:17026,testability,Test,Test,17026,"site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:17077,testability,Trace,Traceback,17077,"rnal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:17854,testability,Trace,Traceback,17854,"_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:18689,testability,trace,trace,18689,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19007,testability,test,testlogs,19007,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19059,testability,test,test,19059,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19064,testability,log,log,19064,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19092,testability,Test,Testing,19092,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19170,testability,Test,Test,19170,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:19235,testability,Trace,Traceback,19235,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:20008,testability,Trace,Traceback,20008,"gs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:20843,testability,trace,trace,20843,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21014,testability,test,tests,21014," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21031,testability,Test,Testing,21031," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21298,testability,test,testlogs,21298,"nal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21350,testability,test,test,21350,"internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21355,testability,log,log,21355,"nal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21383,testability,Test,Testing,21383,"le ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21461,testability,Test,Test,21461,"flow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:21526,testability,Trace,Traceback,21526,"odule('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:22299,testability,Trace,Traceback,22299,"gs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23134,testability,trace,trace,23134,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23433,testability,test,testlogs,23433,"odule>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23468,testability,test,test,23468,"wrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23473,testability,log,log,23473,"tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23501,testability,Test,Testing,23501,"File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23560,testability,Test,Test,23560,"/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23606,testability,Trace,Traceback,23606,"in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:24373,testability,Trace,Traceback,24373,"790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25208,testability,trace,trace,25208,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25525,testability,test,testlogs,25525,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25578,testability,test,test,25578,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25583,testability,log,log,25583,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25611,testability,Test,Testing,25611,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25688,testability,Test,Test,25688,"odule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25752,testability,Trace,Traceback,25752,"root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-pac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:27188,testability,Trace,Traceback,27188,"ariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28023,testability,trace,trace,28023,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28341,testability,test,testlogs,28341,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28393,testability,test,test,28393,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28398,testability,log,log,28398,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28426,testability,Test,Testing,28426,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28504,testability,Test,Test,28504,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28569,testability,Trace,Traceback,28569,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:29342,testability,Trace,Traceback,29342,"gs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30177,testability,trace,trace,30177,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30348,testability,test,tests,30348," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30365,testability,Test,Testing,30365," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30615,testability,test,testlogs,30615,"_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30656,testability,test,test,30656,"module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30661,testability,log,log,30661,"e>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30689,testability,Test,Testing,30689,"l = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30754,testability,Test,Test,30754,"-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30806,testability,Trace,Traceback,30806,"py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:31585,testability,Trace,Traceback,31585,"variant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32420,testability,trace,trace,32420,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e6157205167",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32745,testability,test,testlogs,32745,"thon.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32806,testability,test,test,32806,"l/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32811,testability,log,log,32811,"/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32839,testability,Test,Testing,32839,"rflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32924,testability,Test,Test,32924,"ow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32996,testability,Trace,Traceback,32996,"ackages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:34464,testability,Trace,Traceback,34464,"/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35299,testability,trace,trace,35299,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35617,testability,test,testlogs,35617,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35669,testability,test,test,35669,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35674,testability,log,log,35674,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35702,testability,Test,Testing,35702,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35780,testability,Test,Test,35780,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35845,testability,Trace,Traceback,35845,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:36618,testability,Trace,Traceback,36618,"gs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37453,testability,trace,trace,37453,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37771,testability,test,testlogs,37771,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37823,testability,test,test,37823,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37828,testability,log,log,37828,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37856,testability,Test,Testing,37856,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37934,testability,Test,Test,37934,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37999,testability,Trace,Traceback,37999,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:38772,testability,Trace,Traceback,38772,"gs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39607,testability,trace,trace,39607,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39778,testability,test,tests,39778," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39795,testability,Test,Testing,39795," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40061,testability,test,testlogs,40061,"rnal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40112,testability,test,test,40112,"w_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40117,testability,log,log,40117,"ernal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40145,testability,Test,Testing,40145,"File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40222,testability,Test,Test,40222,"sorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:40286,testability,Trace,Traceback,40286,"ad_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41057,testability,Trace,Traceback,41057,"estlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:hap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41892,testability,trace,trace,41892,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42193,testability,test,testlogs,42193,"ule>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42230,testability,test,test,42230,"_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42235,testability,log,log,42235,"orflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42263,testability,Test,Testing,42263," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42324,testability,Test,Test,42324,"n/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42372,testability,Trace,Traceback,42372,"le>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42671,testability,test,testing,42671," cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:42901,testability,test,testing,42901,"this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:43465,testability,Trace,Traceback,43465,"16790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44300,testability,trace,trace,44300,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44617,testability,test,testlogs,44617,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44668,testability,test,test,44668," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44673,testability,log,log,44673," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44701,testability,Test,Testing,44701,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44778,testability,Test,Test,44778,"<module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44842,testability,Trace,Traceback,44842,"""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:45613,testability,Trace,Traceback,45613,"estlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46448,testability,trace,trace,46448,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46619,testability,test,tests,46619," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46636,testability,Test,Testing,46636," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46901,testability,test,testlogs,46901,"ernal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46952,testability,test,test,46952,"ow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46957,testability,log,log,46957,"ternal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46985,testability,Test,Testing,46985," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:47062,testability,Test,Test,47062,"nsorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:47126,testability,Trace,Traceback,47126,"oad_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:47897,testability,Trace,Traceback,47897,"estlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:48732,testability,trace,trace,48732,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49049,testability,test,testlogs,49049,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49100,testability,test,test,49100," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49105,testability,log,log,49105," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49133,testability,Test,Testing,49133,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49210,testability,Test,Test,49210,"<module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:49274,testability,Trace,Traceback,49274,"""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:50045,testability,Trace,Traceback,50045,"estlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:50880,testability,trace,trace,50880,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disabl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51198,testability,test,testlogs,51198,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Trace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51250,testability,test,test,51250,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51255,testability,log,log,51255,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51283,testability,Test,Testing,51283,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51361,testability,Test,Test,51361,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:51426,testability,Trace,Traceback,51426,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:52197,testability,Trace,Traceback,52197,"logs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53032,testability,trace,trace,53032,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53203,testability,test,tests,53203," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53220,testability,Test,Testing,53220," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53485,testability,test,testlogs,53485,"ernal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53536,testability,test,test,53536,"ow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53541,testability,log,log,53541,"ternal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53569,testability,Test,Testing,53569," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53646,testability,Test,Test,53646,"nsorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53710,testability,Trace,Traceback,53710,"oad_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:54481,testability,Trace,Traceback,54481,"estlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55316,testability,trace,trace,55316,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55633,testability,test,testlogs,55633,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55684,testability,test,test,55684," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55689,testability,log,log,55689," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55717,testability,Test,Testing,55717,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55794,testability,Test,Test,55794,"<module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55858,testability,Trace,Traceback,55858,"""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:56629,testability,Trace,Traceback,56629,"estlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57464,testability,trace,trace,57464,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57781,testability,test,testlogs,57781,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57832,testability,test,test,57832," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57837,testability,log,log,57837," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57865,testability,Test,Testing,57865,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57942,testability,Test,Test,57942,"<module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:58006,testability,Trace,Traceback,58006,"""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:58777,testability,Trace,Traceback,58777,"estlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59612,testability,trace,trace,59612,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59929,testability,test,testlogs,59929,"rflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59980,testability,test,test,59980," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59985,testability,log,log,59985," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:60013,testability,Test,Testing,60013,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:60090,testability,Test,Test,60090,"<module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:60154,testability,Trace,Traceback,60154,"""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:60925,testability,Trace,Traceback,60925,"estlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61760,testability,trace,trace,61760,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61931,testability,test,tests,61931," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61948,testability,Test,Testing,61948," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62215,testability,test,testlogs,62215,"nal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62267,testability,test,test,62267,"internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62272,testability,log,log,62272,"nal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62300,testability,Test,Testing,62300,"le ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62378,testability,Test,Test,62378,"flow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:62443,testability,Trace,Traceback,62443,"odule('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:63216,testability,Trace,Traceback,63216,"gs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64051,testability,trace,trace,64051,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64369,testability,test,testlogs,64369,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64421,testability,test,test,64421,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64426,testability,log,log,64426,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64454,testability,Test,Testing,64454,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64532,testability,Test,Test,64532,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64597,testability,Trace,Traceback,64597,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:65370,testability,Trace,Traceback,65370,"gs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66205,testability,trace,trace,66205,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66523,testability,test,testlogs,66523,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66575,testability,test,test,66575,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66580,testability,log,log,66580,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66608,testability,Test,Testing,66608,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66686,testability,Test,Test,66686,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66751,testability,Trace,Traceback,66751,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:67524,testability,Trace,Traceback,67524,"gs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68359,testability,trace,trace,68359,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68677,testability,test,testlogs,68677,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68729,testability,test,test,68729,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68734,testability,log,log,68734,"/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68762,testability,Test,Testing,68762,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68840,testability,Test,Test,68840,"dule>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68905,testability,Trace,Traceback,68905,"ot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:69678,testability,Trace,Traceback,69678,"gs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70513,testability,trace,trace,70513,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70684,testability,test,tests,70684," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70701,testability,Test,Testing,70701," last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70959,testability,test,testlogs,70959,"low_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71011,testability,test,test,71011,"nsorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71016,testability,log,log,71016,"low_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71134,testability,test,testlogs,71134,"internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71186,testability,test,test,71186," = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71191,testability,log,log,71191,"p.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71309,testability,test,testlogs,71309,"ed object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71361,testability,test,test,71361,"to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71366,testability,log,log,71366,"ad the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71484,testability,test,testlogs,71484,"Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71536,testability,test,test,71536,"essage when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71541,testability,log,log,71541,"e when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71659,testability,test,testlogs,71659,"506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71711,testability,test,test,71711,"epvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71716,testability,log,log,71716,"iant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e61",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71834,testability,test,testlogs,71834,mmary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71886,testability,test,test,71886,4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:71891,testability,log,log,71891,0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72009,testability,test,testlogs,72009,st.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Tr,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72061,testability,test,test,72061,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72066,testability,log,log,72066,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72184,testability,test,testlogs,72184,"st.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72236,testability,test,test,72236,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72241,testability,log,log,72241,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72359,testability,test,testlogs,72359,"st.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72411,testability,test,test,72411,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72416,testability,log,log,72416,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72534,testability,test,testlogs,72534,"st.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72587,testability,test,test,72587,"b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72592,testability,log,log,72592,"e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72780,testability,test,testlogs,72780,"/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72833,testability,test,test,72833,"estlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72838,testability,log,log,72838,"gs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72866,testability,Test,Testing,72866,"t/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:72945,testability,Test,Test,72945,"5720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:73011,testability,Trace,Traceback,73011,"s/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:73784,testability,Trace,Traceback,73784,"deepvariant/model_train_test/shard_10_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74619,testability,trace,trace,74619,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74924,testability,test,testlogs,74924,". from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74965,testability,test,test,74965,"low_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74970,testability,log,log,74970,"nternal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74998,testability,Test,Testing,74998,".local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:75063,testability,Test,Test,75063,"sorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:75115,testability,Trace,Traceback,75115,"sorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:75422,testability,test,testing,75422,"ct file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:75656,testability,test,testing,75656,"ng for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:76220,testability,Trace,Traceback,76220,"root/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77055,testability,trace,trace,77055,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77392,testability,test,testlogs,77392,"tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_uti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77465,testability,test,test,77465,"kages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77470,testability,log,log,77470,"/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77498,testability,Test,Testing,77498,"orflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77595,testability,Test,Test,77595,"File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77679,testability,Trace,Traceback,77679,"rnal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:78049,testability,test,testdata,78049,"k trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:78290,testability,test,testdata,78290,"zel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:78351,testability,test,testing,78351,"_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:78639,testability,test,testing,78639,"ount_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:79203,testability,Trace,Traceback,79203,"le_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>. from third_party.nucleus.testing import test_utils as nucleus_test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80038,testability,trace,trace,80038,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80381,testability,test,testlogs,80381,"flow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80460,testability,test,test,80460,"flow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pyw",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80465,testability,log,log,80465,"python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80493,testability,Test,Testing,80493,"nal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80596,testability,Test,Test,80596,"/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80686,testability,Trace,Traceback,80686,"ig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:81541,testability,Trace,Traceback,81541,"enerate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82376,testability,trace,trace,82376,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82692,testability,test,testlogs,82692,"orflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82744,testability,test,test,82744," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82749,testability,log,log,82749," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82777,testability,Test,Testing,82777,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82853,testability,Test,Test,82853," <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82916,testability,Trace,Traceback,82916,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:84034,testability,Trace,Traceback,84034,"le_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:84869,testability,trace,trace,84869,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85196,testability,test,testlogs,85196,"on.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85259,testability,test,test,85259,"b/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85264,testability,log,log,85264,"hon2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85292,testability,Test,Testing,85292,"w/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85379,testability,Test,Test,85379,"ernal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:85453,testability,Trace,Traceback,85453,"tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:86604,testability,Trace,Traceback,86604,"in/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87439,testability,trace,trace,87439,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87610,testability,test,tests,87610," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87628,testability,Test,Testing,87628,"last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87888,testability,test,testlogs,87888,"_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87934,testability,test,test,87934,"_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87939,testability,log,log,87939,"orflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87967,testability,Test,Testing,87967,"helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:88037,testability,Test,Test,88037,"python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:88094,testability,Trace,Traceback,88094,"rt_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:88883,testability,Trace,Traceback,88883,"testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:89718,testability,trace,trace,89718,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90034,testability,test,testlogs,90034,"orflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90086,testability,test,test,90086," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90091,testability,log,log,90091," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90119,testability,Test,Testing,90119,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90195,testability,Test,Test,90195," <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:90258,testability,Trace,Traceback,90258,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:91376,testability,Trace,Traceback,91376,"le_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92211,testability,trace,trace,92211,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_part",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92514,testability,test,testlogs,92514,"e>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92553,testability,test,test,92553,"sorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92558,testability,log,log,92558,"ow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92586,testability,Test,Testing,92586,"oot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92649,testability,Test,Test,92649,"ap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92699,testability,Trace,Traceback,92699,"wrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:93778,testability,Trace,Traceback,93778,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94613,testability,trace,trace,94613,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94929,testability,test,testlogs,94929,"orflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94981,testability,test,test,94981," File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94986,testability,log,log,94986," ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:95014,testability,Test,Testing,95014,"ite-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:95090,testability,Test,Test,95090," <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:95153,testability,Trace,Traceback,95153,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:96271,testability,Trace,Traceback,96271,"le_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97106,testability,trace,trace,97106,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97425,testability,test,testlogs,97425,"low.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97478,testability,test,test,97478,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/gen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97483,testability,log,log,97483,"oot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97511,testability,Test,Testing,97511,"packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <modu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97590,testability,Test,Test,97590,"e>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97656,testability,Trace,Traceback,97656,"local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:99040,testability,Trace,Traceback,99040,"/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:99875,testability,trace,trace,99875,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100176,testability,test,testlogs,100176,"ule>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100229,testability,test,test,100229,"rnal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100234,testability,log,log,100234,"import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_goog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100352,testability,test,testlogs,100352,"module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100405,testability,test,test,100405,"helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100410,testability,log,log,100410,"r(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100598,testability,test,testlogs,100598,", pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100651,testability,test,test,100651,".9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/gen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100656,testability,log,log,100656," cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100684,testability,Test,Testing,100684,"e: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <modu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100763,testability,Test,Test,100763,"ee https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100829,testability,Trace,Traceback,100829," and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:102213,testability,Trace,Traceback,102213,"/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103048,testability,trace,trace,103048,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103370,testability,test,testlogs,103370,".python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103428,testability,test,test,103428,"/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103433,testability,log,log,103433,"al/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103461,testability,Test,Testing,103461,"/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103543,testability,Test,Test,103543,"_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103612,testability,Trace,Traceback,103612,"on2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:105068,testability,Trace,Traceback,105068,"el-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from third_party.nucleus.io import genomics_reader. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>. from tensorflow.python.lib.io import python_io. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:105903,testability,trace,trace,105903,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106074,testability,test,tests,106074," (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106092,testability,Test,Testing,106092,"last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106346,testability,test,testlogs,106346,"orflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106397,testability,test,test,106397,"p_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106402,testability,log,log,106402,"sorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106520,testability,test,testlogs,106520,"low_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106571,testability,test,test,106571," _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106576,testability,log,log,106576," = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106694,testability,test,testlogs,106694," shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106745,testability,test,test,106745,"ailed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106750,testability,log,log,106750," to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106868,testability,test,testlogs,106868,"ions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106919,testability,test,test,106919,"error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:106924,testability,log,log,106924," message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107042,testability,test,testlogs,107042,"20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107094,testability,test,test,107094,"ing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e61572",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107099,testability,log,log,107099,"/deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e6157205167",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107217,testability,test,testlogs,107217, (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107268,testability,test,test,107268,1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eva,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107273,testability,log,log,107273,4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_tes,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107391,testability,test,testlogs,107391,0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107442,testability,test,test,107442,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107447,testability,log,log,107447,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107565,testability,test,testlogs,107565,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107616,testability,test,test,107616,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107621,testability,log,log,107621,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107739,testability,test,testlogs,107739,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107790,testability,test,test,107790,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107795,testability,log,log,107795,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107913,testability,test,testlogs,107913,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107964,testability,test,test,107964,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:107969,testability,log,log,107969,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108155,testability,test,testlogs,108155,"516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108206,testability,test,test,108206,"t/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108211,testability,log,log,108211,"opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108239,testability,Test,Testing,108239,"_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108316,testability,Test,Test,108316,"024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:108380,testability,Trace,Traceback,108380,"8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:109151,testability,Trace,Traceback,109151,"estlogs/deepvariant/model_eval_test/shard_9_of_10/test.log). (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:109986,testability,trace,trace,109986,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110285,testability,test,testlogs,110285,"odule>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110320,testability,test,test,110320,"wrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110325,testability,log,log,110325,"tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110353,testability,Test,Testing,110353,"File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110412,testability,Test,Test,110412,"/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110458,testability,Trace,Traceback,110458,"in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:111225,testability,Trace,Traceback,111225,"790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112060,testability,trace,trace,112060,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112378,testability,test,testlogs,112378,"flow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112432,testability,test,test,112432,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112437,testability,log,log,112437,"oot/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112465,testability,Test,Testing,112465,"packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112543,testability,Test,Test,112543,"le>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112608,testability,Trace,Traceback,112608,"/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:113732,testability,Trace,Traceback,113732,"pvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. from tensorflow.python.platform import gfile. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:pos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114567,testability,trace,trace,114567,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114878,testability,test,testlogs,114878," tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Trac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114925,testability,test,test,114925," import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114930,testability,log,log,114930,"rt *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114958,testability,Test,Testing,114958,"ython2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:115029,testability,Test,Test,115029,"y"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:115087,testability,Trace,Traceback,115087,"import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:115878,testability,Trace,Traceback,115878,"ogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:116713,testability,trace,trace,116713,"ap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s. (06:29:21) INFO: 43 processes: 43 local. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. //deepvariant:allelecounter_test (cached) PASSED in 0.5s. //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:116990,testability,test,tests,116990,"low.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s. (06:29:21) INFO: 43 processes: 43 local. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. //deepvariant:allelecounter_test (cached) PASSED in 0.5s. //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118081,testability,test,testlogs,118081,pvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118121,testability,test,test,118121,) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-ou,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118126,testability,log,log,118126,SED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118294,testability,test,testlogs,118294, (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118335,testability,test,test,118335,:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-ou,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118340,testability,log,log,118340,s_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118504,testability,test,testlogs,118504,deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118541,testability,test,test,118541,buf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118546,testability,log,log,118546,mplementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118708,testability,test,testlogs,118708, in 0.5s. //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118743,testability,test,test,118743,/python:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execro,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118748,testability,log,log,118748,on:ssw_misc_test (cached) PASSED in 0.9s. //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s. //deepvariant/vendor:timer_test (cached) PASSED in 0.7s. //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/co,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118914,testability,test,testlogs,118914, //deepvariant:call_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118953,testability,test,test,118953,LED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce69,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:118958,testability,log,log,118958,n 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log. //deepvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119132,testability,test,testlogs,119132,pvariant:data_providers_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119179,testability,test,test,119179,/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119184,testability,log,log,119184,/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log. //deepvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_baze,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119346,testability,test,testlogs,119346,pvariant:haplotypes_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepv,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119381,testability,test,test,119381,n 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119386,testability,log,log,119386,s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log. //deepvariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119554,testability,test,testlogs,119554,ariant:modeling_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119595,testability,test,test,119595,oot/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/la,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119600,testability,log,log,119600,cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log. //deepvariant:pileup_image_test FAILED in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119788,testability,test,testlogs,119788,in 0.3s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_la,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119849,testability,test,test,119849,15720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wra,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:119854,testability,log,log,119854,516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log. //deepvariant:postprocess_variants_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_tes,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120033,testability,test,testlogs,120033,/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_tes,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120085,testability,test,test,120085,root/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_w,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120090,testability,log,log,120090,com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log. //deepvariant:tf_utils_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_t,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120275,testability,test,testlogs,120275,b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120333,testability,test,test,120333,out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120338,testability,log,log,120338,8-opt/testlogs/deepvariant/tf_utils_test/test.log. //deepvariant:variant_caller_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120518,testability,test,testlogs,120518,le_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120571,testability,test,test,120571,t/variant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.ca,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120576,testability,log,log,120576,iant_caller_test/test.log. //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/b,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120753,testability,test,testlogs,120753,e_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120803,testability,test,test,120803,ant/labeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cach,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120808,testability,log,log,120808,abeler/customized_classes_labeler_test/test.log. //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/baz,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:120987,testability,test,testlogs,120987,t/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigne,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121039,testability,test,test,121039,/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121044,testability,log,log,121044,variant/labeler/haplotype_labeler_test/test.log. //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILE,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121225,testability,test,testlogs,121225,croot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121279,testability,test,test,121279,gs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigne,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121284,testability,log,log,121284,epvariant/labeler/labeled_examples_to_vcf_test/test.log. //deepvariant/labeler:positional_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/all,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121455,testability,test,testlogs,121455,0516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testl,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121499,testability,test,test,121499,bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_lin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121504,testability,log,log,121504,-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log. //deepvariant/labeler:variant_labeler_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121677,testability,test,testlogs,121677,24b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121723,testability,test,test,121723,eepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/tes,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121728,testability,log,log,121728,riant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log. //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121907,testability,test,testlogs,121907,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121959,testability,test,test,121959,"pvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:121964,testability,log,log,121964,"ant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log. //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122170,testability,test,testlogs,122170,"5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122249,testability,test,test,122249,"n/variant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122254,testability,log,log,122254,"iant_calling_wrap_test/test.log. //deepvariant/realigner:aligner_test FAILED in 0.4s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122454,testability,test,testlogs,122454,"tlogs/deepvariant/realigner/aligner_test/test.log. //deepvariant/realigner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e61",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122527,testability,test,test,122527,"ner:realigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122532,testability,log,log,122532,"ealigner_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122722,testability,test,testlogs,122722,"t.log. //deepvariant/realigner:window_selector_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122785,testability,test,test,122785," 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:122790,testability,log,log,122790,". /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log. //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123038,testability,test,testlogs,123038,"AILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123091,testability,test,test,123091,"a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123096,testability,log,log,123096,"24b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123214,testability,test,testlogs,123214,"ar/generate_trained_model_test/test.log. //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123267,testability,test,test,123267,"ant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123272,testability,log,log,123272,"ealigner/allele_count_linear:model_evaluation_test FAILED in 0.5s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123520,testability,test,testlogs,123520,"st/test.log. //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123571,testability,test,test,123571,"ijn_graph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123576,testability,log,log,123576,"raph_wrap_test FAILED in 0.2s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123694,testability,test,testlogs,123694,"ariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123745,testability,test,test,123745,"igner/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123750,testability,log,log,123750,"/python/debruijn_graph_wrap_test/test.log. //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s. Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123868,testability,test,testlogs,123868,"uns: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123919,testability,test,test,123919,"0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:123924,testability,log,log,123924,"root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124042,testability,test,testlogs,124042,"/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124093,testability,test,test,124093,".log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124098,testability,log,log,124098," /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124216,testability,test,testlogs,124216,"gs/deepvariant/make_examples_test/shard_1_of_2/test.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats ove",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124268,testability,test,test,124268,"t.log. //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124273,testability,log,log,124273,". //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124391,testability,test,testlogs,124391,"dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124442,testability,test,test,124442,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124447,testability,log,log,124447,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124565,testability,test,testlogs,124565,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124616,testability,test,test,124616,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124621,testability,log,log,124621,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124739,testability,test,testlogs,124739,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124790,testability,test,test,124790,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124795,testability,log,log,124795,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124913,testability,test,testlogs,124913,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124964,testability,test,test,124964,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:124969,testability,log,log,124969,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125087,testability,test,testlogs,125087,"0/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125138,testability,test,test,125138,"1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125143,testability,log,log,125143,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125392,testability,test,testlogs,125392,"ogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125444,testability,test,test,125444,".log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125449,testability,log,log,125449," /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125567,testability,test,testlogs,125567,"gs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125619,testability,test,test,125619,"log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125624,testability,log,log,125624,"/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125742,testability,test,testlogs,125742,"s/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125794,testability,test,test,125794,"og. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125799,testability,log,log,125799,"root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125917,testability,test,testlogs,125917,"/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125969,testability,test,test,125969,"g. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:125974,testability,log,log,125974,"oot/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126092,testability,test,testlogs,126092,"deepvariant/model_eval_test/shard_9_of_10/test.log. //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. Th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126144,testability,test,test,126144,". //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126149,testability,log,log,126149,"eepvariant:model_train_test FAILED in 10 out of 10 in 0.4s. Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126267,testability,test,testlogs,126267,"= 0.0s. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126319,testability,test,test,126319,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126324,testability,log,log,126324,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126442,testability,test,testlogs,126442,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126494,testability,test,test,126494,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126499,testability,log,log,126499,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126617,testability,test,testlogs,126617,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126669,testability,test,test,126669,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126674,testability,log,log,126674,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126792,testability,test,testlogs,126792,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126844,testability,test,test,126844,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126849,testability,log,log,126849,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:126967,testability,test,testlogs,126967,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127020,testability,test,test,127020,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127025,testability,log,log,127025,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127052,testability,test,tests,127052,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127062,testability,test,tests,127062,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127105,testability,test,tests,127105,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127270,testability,test,tests,127270,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41,usability,help,help,41,"Hello @sidharthgoel . Thank you for your help with this issue - I was able to build deepvariant! Tests failed, below, and I am happy to open a separate issue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```. FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9472,usability,command,command,9472,"ZED_TF_WHL_FILENAME=tensorflow-1.12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9505,usability,command,commands,9505,"12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9697,usability,stop,stops,9697,"riant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a lis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:9994,usability,help,help,9994," ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10006,usability,help,help,10006,"V_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10015,usability,command,commands,10015,"PU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. +++ which python. ++ export PYTHON_BIN_PATH=/usr/bin/python. ++ PYTHON_BIN_PATH=/usr/bin/python. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10210,usability,command,command,10210,"_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10331,usability,Stop,Stops,10331,"+ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'. + bazel. [bazel release 0.15.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-statu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10463,usability,help,help,10463,"bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e6157205167",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10476,usability,help,help,10476,"d> <options> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10482,usability,command,command,10482,"ons> ... Available commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10499,usability,help,help,10499,"ble commands:. analyze-profile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10521,usability,command,command,10521,"rofile Analyzes build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10537,usability,help,help,10537,"s build profile data. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10600,usability,help,help,10600,"icalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10670,usability,help,help,10670,"put files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. =================",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:10727,usability,command,command,10727,"es code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11035,usability,behavi,behavior,11035,"info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. fr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:11329,usability,statu,status,11329,"tops the bazel server. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. (06:29:06) INFO: Current date is 2019-02-14. (06:29:06) Loading: . (06:29:06) Loading: 0 packages loaded. (06:29:07) INFO: Analysed 168 targets (0 packages loaded). (06:29:07) INFO: Found 130 targets and 38 test targets... (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log). (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:. ==================== Test output for //deepvariant/labeler:variant_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>. from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:13924,usability,error,errors,13924,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14014,usability,error,error,14014,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:14044,usability,help,help,14044,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log). (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:. ==================== Test output for //deepvariant/realigner:aligner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16381,usability,error,errors,16381,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. Fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16471,usability,error,error,16471,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:16501,usability,help,help,16501,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running). (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log). (06:29:09) INFO: From Testing //deepvariant:call_variants_test:. ==================== Test output for //deepvariant:call_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:18617,usability,error,errors,18617,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:18707,usability,error,error,18707,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:18737,usability,help,help,18737,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:20771,usability,error,errors,20771,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:20861,usability,error,error,20861,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:20891,usability,help,help,20891,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running). (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log). (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23062,usability,error,errors,23062,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23152,usability,error,error,23152,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:23182,usability,help,help,23182,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log). (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:. ==================== Test output for //deepvariant:tf_utils_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25136,usability,error,errors,25136,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce69",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25226,usability,error,error,25226,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:25256,usability,help,help,25256,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:. ==================== Test output for //deepvariant/labeler:positional_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labele",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:27951,usability,error,errors,27951,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28041,usability,error,error,28041,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:28071,usability,help,help,28071,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log). (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30105,usability,error,errors,30105,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30195,usability,error,error,30195,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:30225,usability,help,help,30225,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running). (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log). (06:29:10) INFO: From Testing //deepvariant:data_providers_test:. ==================== Test output for //deepvariant:data_providers_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32348,usability,error,errors,32348,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32438,usability,error,error,32438,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:32468,usability,help,help,32468,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log). (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:. ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35227,usability,error,errors,35227,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35317,usability,error,error,35317,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:35347,usability,help,help,35347,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37381,usability,error,errors,37381,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37471,usability,error,error,37471,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:37501,usability,help,help,37501,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log). (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39535,usability,error,errors,39535,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39625,usability,error,error,39625,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:39655,usability,help,help,39655,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running). (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41820,usability,error,errors,41820,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41910,usability,error,error,41910,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:41940,usability,help,help,41940,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log). (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:. ==================== Test output for //deepvariant:haplotypes_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <modu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44228,usability,error,errors,44228,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <modul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44318,usability,error,error,44318,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:44348,usability,help,help,44348,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log). (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46376,usability,error,errors,46376,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46466,usability,error,error,46466,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:46496,usability,help,help,46496,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running). (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:48660,usability,error,errors,48660,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <modul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:48750,usability,error,error,48750,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:48780,usability,help,help,48780,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:50808,usability,error,errors,50808,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:50898,usability,error,error,50898,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:50928,usability,help,help,50928,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log). (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:52960,usability,error,errors,52960,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53050,usability,error,error,53050,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:53080,usability,help,help,53080,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running). (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55244,usability,error,errors,55244,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <modul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55334,usability,error,error,55334,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:55364,usability,help,help,55364,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57392,usability,error,errors,57392,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <modul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57482,usability,error,error,57482,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:57512,usability,help,help,57512,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log). (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59540,usability,error,errors,59540,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <modul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59630,usability,error,error,59630,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:59660,usability,help,help,59660,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):. ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61688,usability,error,errors,61688,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61778,usability,error,error,61778,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61808,usability,help,help,61808,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running). (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:63979,usability,error,errors,63979,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64069,usability,error,error,64069,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:64099,usability,help,help,64099,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log). (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66133,usability,error,errors,66133,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66223,usability,error,error,66223,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:66253,usability,help,help,66253,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68287,usability,error,errors,68287,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68377,usability,error,error,68377,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:68407,usability,help,help,68407,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log). (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):. ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70441,usability,error,errors,70441,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70531,usability,error,error,70531,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:70561,usability,help,help,70561,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74547,usability,error,errors,74547,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74637,usability,error,error,74637,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/thir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:74667,usability,help,help,74667,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log). (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:. ==================== Test output for //deepvariant:variant_caller_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>. from third_party.nucleus.testing import test_utils. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:76983,usability,error,errors,76983,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_eval",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77073,usability,error,error,77073,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cach",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:77103,usability,help,help,77103,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log). (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>. from deepvariant import testdata. File ""/root/.cache/bazel/_bazel_root/ce699a1ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:79966,usability,error,errors,79966,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80056,usability,error,error,80056,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:80086,usability,help,help,80086,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>. import tensorflow as tf. File ""/root/.local/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82304,usability,error,errors,82304,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82394,usability,error,error,82394,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:82424,usability,help,help,82424,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:. ==================== Test output for //deepvariant/python:allelecounter_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:84797,usability,error,errors,84797,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:84887,usability,error,error,84887,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:84917,usability,help,help,84917,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:. ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87367,usability,error,errors,87367,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87457,usability,error,error,87457,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:87487,usability,help,help,87487,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running). (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:. ==================== Test output for //deepvariant/realigner:realigner_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:89646,usability,error,errors,89646,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:89736,usability,error,error,89736,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:89766,usability,help,help,89766,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log). (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:. ==================== Test output for //deepvariant/realigner:window_selector_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92139,usability,error,errors,92139,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92229,usability,error,error,92229,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:92259,usability,help,help,92259,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log). (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:. ==================== Test output for //deepvariant:pileup_image_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94541,usability,error,errors,94541,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94631,usability,error,error,94631,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:94661,usability,help,help,94661,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log). (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:. ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97034,usability,error,errors,97034,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97124,usability,error,error,97124,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:97154,usability,help,help,97154,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log). (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:99803,usability,error,errors,99803,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:99893,usability,error,error,99893,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:99923,usability,help,help,99923,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. FAILED: //deepvariant:make_examples_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log. (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log). (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):. ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e61572",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:102976,usability,error,errors,102976,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103066,usability,error,error,103066,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:103096,usability,help,help,103096,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log). (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:. ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>. from third_party.nucleus.io import vcf. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:105831,usability,error,errors,105831,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:105921,usability,error,error,105921,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:105951,usability,help,help,105951,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary). /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log. /root/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:109914,usability,error,errors,109914,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110004,usability,error,error,110004,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:110034,usability,help,help,110034,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log). (06:29:20) INFO: From Testing //deepvariant:modeling_test:. ==================== Test output for //deepvariant:modeling_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>. from te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:111988,usability,error,errors,111988,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112078,usability,error,error,112078,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:112108,usability,help,help,112108,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log). (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:. ==================== Test output for //deepvariant/python:variant_calling_wrap_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>. from third_party.nucleus.io import fasta. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114495,usability,error,errors,114495,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114585,usability,error,error,114585,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:114615,usability,help,help,114615,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log). (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:. ==================== Test output for //deepvariant:postprocess_variants_test:. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>. import tensorflow as tf. File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:116641,usability,error,errors,116641,"n/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s. (06:29:21) INFO: 43 processes: 43 local. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. //deepvariant:allelecounter_test (cached) PASSED in 0.5s. //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cach",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:116731,usability,error,error,116731,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s. (06:29:21) INFO: 43 processes: 43 local. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. //deepvariant:allelecounter_test (cached) PASSED in 0.5s. //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:116761,usability,help,help,116761,"/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace. above this error message when asking for help. ================================================================================. (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s. (06:29:21) INFO: 43 processes: 43 local. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. //deepvariant:allelecounter_test (cached) PASSED in 0.5s. //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s. //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s. //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s. //deepvariant:resources_test (cached) PASSED in 1.7s. //deepvariant:utils_test (cached) PASSED in 0.5s. //deepvariant:variant_calling_test (cached) PASSED in 0.6s. //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s. //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s. //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s. //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s. //deepvariant/realigner/python:ssw_misc_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:127184,usability,command,command,127184,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log. /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally. There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are. (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:151,deployability,version,version,151,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:260,deployability,automat,automatically,260,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:274,deployability,instal,installs,274,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:0,energy efficiency,Current,Currently,0,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:151,integrability,version,version,151,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:131,interoperability,specif,specifying,131,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:151,modifiability,version,version,151,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:215,reliability,doe,doesn,215,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:347,security,modif,modify,347,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:260,testability,automat,automatically,260,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:146,deployability,instal,install,146,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:180,deployability,build,building,180,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:100,energy efficiency,current,currently,100,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:230,modifiability,variab,variable,230,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:403,testability,simpl,simplify,403,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:403,usability,simpl,simplify,403,"Hi @andrewrech . I'll be closing this issue. To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/146:110,availability,error,error,110,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:116,integrability,messag,message,116,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:116,interoperability,messag,message,116,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:110,performance,error,error,110,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:110,safety,error,error,110,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:110,usability,error,error,110,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:35,deployability,build,building,35,Thank you @gunjanbaid . I was just building DV from source. I realized the path was wrong and just corrected for my build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:116,deployability,build,build,116,Thank you @gunjanbaid . I was just building DV from source. I realized the path was wrong and just corrected for my build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:190,deployability,build,build,190,"@andrewrech The file has been moved to `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`, as expected by the script. . You may want to change your build path back to the original path. The file at `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz` will eventually be removed. Thanks again for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:329,integrability,event,eventually,329,"@andrewrech The file has been moved to `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`, as expected by the script. . You may want to change your build path back to the original path. The file at `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz` will eventually be removed. Thanks again for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:83,modifiability,pac,packages,83,"@andrewrech The file has been moved to `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`, as expected by the script. . You may want to change your build path back to the original path. The file at `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz` will eventually be removed. Thanks again for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:284,modifiability,pac,packages,284,"@andrewrech The file has been moved to `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`, as expected by the script. . You may want to change your build path back to the original path. The file at `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz` will eventually be removed. Thanks again for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/147:171,deployability,version,versions,171,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:171,integrability,version,versions,171,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:171,modifiability,version,versions,171,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:138,performance,performance analys,performance analysis,138,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:138,usability,perform,performance,138,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:239,usability,help,helps,239,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:83,deployability,depend,dependent,83,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:136,energy efficiency,CPU,CPU,136,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:83,integrability,depend,dependent,83,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:83,modifiability,depend,dependent,83,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:78,performance,time,time-dependent,78,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:104,performance,memor,memory,104,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:136,performance,CPU,CPU,136,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:83,safety,depend,dependent,83,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:83,testability,depend,dependent,83,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:104,usability,memor,memory,104,"Hi Paul,. Thanks for your kind comment. I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:491,availability,slo,slow,491,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:856,deployability,build,build,856,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:884,deployability,pipelin,pipeline,884,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:36,energy efficiency,Current,Currently,36,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:264,energy efficiency,CPU,CPU,264,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:391,energy efficiency,CPU,CPU,391,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:565,energy efficiency,Cloud,Cloud,565,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:607,energy efficiency,cloud,cloud,607,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:661,energy efficiency,Current,Currently,661,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:722,energy efficiency,model,models,722,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:738,energy efficiency,Cloud,Cloud,738,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:951,energy efficiency,Cloud,Cloud,951,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:410,integrability,batch,batch,410,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:884,integrability,pipelin,pipeline,884,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:443,modifiability,paramet,parameters,443,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:264,performance,CPU,CPU,264,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:391,performance,CPU,CPU,391,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:410,performance,batch,batch,410,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:491,reliability,slo,slow,491,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:502,reliability,pra,practical,502,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:311,security,team,team,311,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:571,security,team,team,571,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:722,security,model,models,722,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:744,security,team,team,744,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:957,security,team,team,957,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1109,security,team,team,1109,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:771,testability,understand,understand,771,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:828,testability,understand,understand,828,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:999,usability,close,close,999,"Hi @koido . thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:. You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant. Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/149:785,deployability,configurat,configuration,785,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:866,deployability,pipelin,pipeline,866,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:998,deployability,pipelin,pipeline,998,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:921,energy efficiency,Cloud,Cloud,921,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:785,integrability,configur,configuration,785,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:866,integrability,pipelin,pipeline,866,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:998,integrability,pipelin,pipeline,998,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:274,interoperability,specif,specify,274,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:773,interoperability,share,shared,773,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:785,modifiability,configur,configuration,785,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:495,reliability,doe,does,495,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:960,reliability,doe,does,960,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:449,safety,test,test,449,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1,security,modif,modified,1,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:785,security,configur,configuration,785,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:449,testability,test,test,449,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:942,usability,command,command,942,"(modified from earlier response). 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file. * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai. ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:50,deployability,configurat,configuration,50,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:80,deployability,pipelin,pipeline,80,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:50,integrability,configur,configuration,50,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:80,integrability,pipelin,pipeline,80,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:50,modifiability,configur,configuration,50,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:50,security,configur,configuration,50,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:132,testability,simpl,simply,132,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:132,usability,simpl,simply,132,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:20,reliability,doe,does,20,I think it actually does work with simply reads.bam and reads.bai. Yep!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:35,testability,simpl,simply,35,I think it actually does work with simply reads.bam and reads.bai. Yep!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:35,usability,simpl,simply,35,I think it actually does work with simply reads.bam and reads.bai. Yep!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:63,security,modif,modify,63,"@ekofman You are correct, thanks for pointing that out! I will modify my earlier response. Index files named as `<NAME>.bam.bai` **AND** `<NAME>.bai` are accepted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/150:684,availability,error,errors,684,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:785,availability,error,errors,785,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:22,deployability,configurat,configurations,22,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:227,deployability,build,build,227,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:237,deployability,Pipelin,Pipelines,237,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:247,deployability,API,API,247,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:129,energy efficiency,cloud,cloud,129,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:632,energy efficiency,core,core,632,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:708,energy efficiency,Current,Currently,708,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:22,integrability,configur,configurations,22,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:237,integrability,Pipelin,Pipelines,237,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:247,integrability,API,API,247,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1038,integrability,discover,discover,1038,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:79,interoperability,specif,specifying,79,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:247,interoperability,API,API,247,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:350,interoperability,format,format,350,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:758,interoperability,format,format,758,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1038,interoperability,discover,discover,1038,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:22,modifiability,configur,configurations,22,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:684,performance,error,errors,684,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:785,performance,error,errors,785,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:937,performance,time,time,937,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:684,safety,error,errors,684,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:785,safety,error,errors,785,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:957,safety,compl,complete,957,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:22,security,configur,configurations,22,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:957,security,compl,complete,957,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:966,security,control,control,966,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:966,testability,control,control,966,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:282,usability,custom,custom,282,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:334,usability,custom,custom,334,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:684,usability,error,errors,684,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:723,usability,support,supports,723,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:785,usability,error,errors,785,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:792,usability,Command,CommandLineError,792,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1038,usability,discov,discover,1038,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1268,usability,help,helps,1268,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well? https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```. machine_type = 'custom-{0}-{1}'.format(. pipeline_args.make_examples_cores_per_worker,. pipeline_args.make_examples_ram_per_worker_gb * 1024). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```. if options.n_cores != 1:. errors.log_and_raise(. 'Currently only supports n_cores == 1 but got {}.'.format(. options.n_cores), errors.CommandLineError). ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:681,deployability,resourc,resource,681,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:907,deployability,observ,observe,907,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,deployability,resourc,resources,1033,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1076,deployability,resourc,resource,1076,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1567,deployability,orchestr,orchestrate,1567,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:146,energy efficiency,core,core,146,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:420,energy efficiency,core,core,420,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:626,energy efficiency,core,cores,626,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:681,energy efficiency,resourc,resource,681,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:959,energy efficiency,core,cores,959,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,energy efficiency,resourc,resources,1033,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1076,energy efficiency,resourc,resource,1076,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1621,energy efficiency,Cloud,Cloud,1621,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1783,energy efficiency,Cloud,Cloud,1783,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1170,integrability,pub,public,1170,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1567,integrability,orchestr,orchestrate,1567,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:167,interoperability,specif,specify,167,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1434,modifiability,evolv,evolving,1434,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:214,performance,parallel,parallel,214,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:260,performance,time,time,260,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:326,performance,parallel,parallel,326,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:480,performance,time,time,480,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:681,performance,resourc,resource,681,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:799,performance,parallel,parallel,799,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,performance,resourc,resources,1033,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1076,performance,resourc,resource,1076,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1448,performance,time,time,1448,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:681,safety,resourc,resource,681,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,safety,resourc,resources,1033,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1076,safety,resourc,resource,1076,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1206,safety,test,test,1206,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1789,security,team,team,1789,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:681,testability,resourc,resource,681,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:907,testability,observ,observe,907,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,testability,resourc,resources,1033,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1076,testability,resourc,resource,1076,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1206,testability,test,test,1206,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:115,usability,experien,experience,115,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:500,usability,confirm,confirm,500,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:843,usability,experien,experience,843,"Hi @ekofman , . thanks for reporting this! It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? . In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past). @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks! The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need. I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:63,deployability,build,building,63,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:163,energy efficiency,core,cores,163,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:24,usability,feedback,feedback,24,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:124,usability,command,command,124,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:206,availability,avail,available,206,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:4,energy efficiency,current,currently,4,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:151,energy efficiency,core,cores,151,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:270,energy efficiency,CPU,CPU,270,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:329,energy efficiency,CPU,CPU,329,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:349,energy efficiency,CPU,CPU,349,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:382,energy efficiency,core,core,382,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:391,energy efficiency,Core,Core,391,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:470,energy efficiency,CPU,CPU,470,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:485,energy efficiency,Model,Model,485,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:496,energy efficiency,Model,Model,496,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:525,energy efficiency,CPU,CPU,525,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:553,energy efficiency,CPU,CPU,553,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:719,energy efficiency,CPU,CPU,719,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1033,energy efficiency,core,core,1033,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1080,energy efficiency,core,core,1080,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:752,integrability,messag,messages,752,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:248,interoperability,Architectur,Architecture,248,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:403,interoperability,socket,socket,403,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:414,interoperability,Socket,Socket,414,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:752,interoperability,messag,messages,752,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:103,performance,time,time,103,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:270,performance,CPU,CPU,270,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:329,performance,CPU,CPU,329,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:349,performance,CPU,CPU,349,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:470,performance,CPU,CPU,470,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:525,performance,CPU,CPU,525,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:553,performance,CPU,CPU,553,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:646,performance,cach,cache,646,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:662,performance,cach,cache,662,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:677,performance,cach,cache,677,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:693,performance,cach,cache,693,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:719,performance,CPU,CPU,719,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1016,performance,memor,memory,1016,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:206,reliability,availab,available,206,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:14,safety,test,testing,14,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:206,safety,avail,available,206,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:206,security,availab,available,206,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:403,security,soc,socket,403,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:414,security,Soc,Socket,414,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:485,security,Model,Model,485,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:496,security,Model,Model,496,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:14,testability,test,testing,14,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:1016,usability,memor,memory,1016,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 63. Model name: Intel(R) Xeon(R) CPU @ 2.30GHz. Stepping: 0. CPU MHz: 2300.000. BogoMIPS: 4600.00. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 256K. L3 cache: 46080K. NUMA node0 CPU(s): 0-15. ```. I then get 16 messages like this:. `. Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:164,energy efficiency,core,cores,164,"@pichuan I'm wondering how much the coverage of the bam should affect the runtime, specifically the set up for each shard. For example, is this now working with 16 cores and shards just because my file is only 2GB, whereas my real data I'd like to be able to run this on is ~250GB.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:83,interoperability,specif,specifically,83,"@pichuan I'm wondering how much the coverage of the bam should affect the runtime, specifically the set up for each shard. For example, is this now working with 16 cores and shards just because my file is only 2GB, whereas my real data I'd like to be able to run this on is ~250GB.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:36,testability,coverag,coverage,36,"@pichuan I'm wondering how much the coverage of the bam should affect the runtime, specifically the set up for each shard. For example, is this now working with 16 cores and shards just because my file is only 2GB, whereas my real data I'd like to be able to run this on is ~250GB.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:377,availability,operat,operations,377,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:24,deployability,updat,update,24,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:270,deployability,build,build,270,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:327,integrability,sub,subset,327,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:590,integrability,pub,public,590,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:119,performance,time,time,119,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:161,performance,time,time,161,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:350,performance,perform,performing,350,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:24,safety,updat,update,24,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:24,security,updat,update,24,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:550,security,ident,identify,550,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:623,testability,understand,understand,623,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:350,usability,perform,performing,350,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:400,usability,minim,minimize,400,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc. But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad. Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/151:752,availability,restor,restore,752,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:760,availability,operat,operator,760,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:334,energy efficiency,model,model,334,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:407,energy efficiency,model,model,407,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:480,energy efficiency,core,core,480,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:594,energy efficiency,model,model,594,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:788,energy efficiency,model,model,788,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:815,energy efficiency,model,models,815,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:911,energy efficiency,model,model,911,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1056,energy efficiency,model,models,1056,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:713,interoperability,format,format,713,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:752,reliability,restor,restore,752,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:334,security,model,model,334,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:407,security,model,model,407,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:594,security,model,model,594,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:631,security,loss,loss,631,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:788,security,model,model,788,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:815,security,model,models,815,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:911,security,model,model,911,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1056,security,model,models,1056,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:19,usability,close,close,19,"I feel like I'm so close... going ahead with the option of running one call_variants per machine per shard and then trying to gather all results before postprocessing. Not sure if that is correct, however, assuming that it is, I'm still now running into this:. I0206 23:26:03.790093 139996085663488 call_variants.py:328] Initializing model from DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001. 2019-02-06 23:26:03.791839: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? I'm using the wgs model found at deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/, called model.ckpt.data-00000-of-00001 .... should I be referencing the full name or just moel.ckpt.data as suggested here https://github.com/tensorflow/models/issues/2676 ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:89,availability,consist,consistent,89,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:42,energy efficiency,model,model,42,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,energy efficiency,model,models,138,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:188,energy efficiency,model,models,188,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:312,energy efficiency,model,models,312,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:386,energy efficiency,model,model,386,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:462,energy efficiency,model,models,462,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:536,energy efficiency,model,model,536,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:601,energy efficiency,model,models,601,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:675,energy efficiency,model,model,675,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:42,security,model,model,42,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,security,model,models,138,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:188,security,model,models,188,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:312,security,model,models,312,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:386,security,model,model,386,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:462,security,model,models,462,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:536,security,model,model,536,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:601,security,model,models,601,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:675,security,model,model,675,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:8,usability,close,close,8,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:89,usability,consist,consistent,89,"Sort of close...why are you using a 0.6.0 model with a 0.7.2 codebase? It's better to be consistent. Here's the location of the 0.7.2 wgs models:. ```Bash. $ gsutil ls -l gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/. 348681272 2018-12-11T23:39:18Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.data-00000-of-00001. 18496 2018-12-11T23:39:42Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.index. 31106596 2018-12-11T23:39:45Z gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard/model.ckpt.meta. TOTAL: 3 objects, 379806364 bytes (362.21 MiB). $. ```. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:198,deployability,pipelin,pipeline,198,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:96,energy efficiency,cloud,cloud,96,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:246,energy efficiency,cloud,cloud,246,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:338,energy efficiency,cloud,cloud,338,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:198,integrability,pipelin,pipeline,198,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:28,safety,test,test,28,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:8,testability,verif,verify,8,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:28,testability,test,test,28,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:853,deployability,version,version,853,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1353,deployability,pipelin,pipeline,1353,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:109,energy efficiency,GPU,GPU,109,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:665,energy efficiency,cloud,cloud,665,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:920,energy efficiency,current,currently,920,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1048,energy efficiency,cloud,cloud,1048,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1251,energy efficiency,cloud,cloud,1251,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1407,energy efficiency,cloud,cloud,1407,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1505,energy efficiency,cloud,cloud,1505,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:706,integrability,compon,components,706,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:853,integrability,version,version,853,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1353,integrability,pipelin,pipeline,1353,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:295,interoperability,format,format,295,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:332,interoperability,specif,specify,332,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:379,interoperability,format,format,379,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:706,interoperability,compon,components,706,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:706,modifiability,compon,components,706,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:853,modifiability,version,version,853,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:109,performance,GPU,GPU,109,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:348,reliability,doe,does,348,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1177,safety,test,test,1177,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:721,security,expos,exposed,721,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1565,security,auth,authored,1565,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:635,testability,understand,understand,635,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1157,testability,verif,verify,1157,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1177,testability,test,test,1177,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:15,usability,help,help,15,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:506,usability,document,documented,506,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:598,usability,document,documentation,598,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:906,usability,workflow,workflows,906,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1100,usability,Mous,Mousavi,1100,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:27,availability,consist,consistent,27,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:483,deployability,pipelin,pipeline,483,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:38,energy efficiency,model,model,38,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:113,energy efficiency,model,model,113,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:381,energy efficiency,cloud,cloud,381,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:537,energy efficiency,cloud,cloud,537,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:635,energy efficiency,cloud,cloud,635,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:483,integrability,pipelin,pipeline,483,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:307,safety,test,test,307,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:38,security,model,model,38,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:113,security,model,model,113,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:695,security,auth,authored,695,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:127,testability,simpl,simply,127,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:287,testability,verif,verify,287,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:307,testability,test,test,307,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:27,usability,consist,consistent,27,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:127,usability,simpl,simply,127,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:230,usability,Mous,Mousavi,230,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:. > . > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path? > . > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. > . > https://cloud.google.com/genomics/docs/tutorials/deepvariant. > . > Is there any reason why you don't use cloud runner? > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1931,availability,Down,Download,1931,"rent from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2048,availability,Down,Download,2048,"v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3836,availability,failur,failure,3836,".gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. Wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4235,availability,failur,failure,4235,"008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. Va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4287,availability,error,errors,4287,"les.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the patte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4416,availability,error,error,4416,"{N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4770,availability,checkpoint,checkpoint,4770," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5049,availability,error,errors,5049," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5204,availability,error,errors,5204," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1972,deployability,updat,update,1972,"posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your origin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1996,deployability,instal,install,1996,"sing that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2030,deployability,instal,install,2030,"ou're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3478,deployability,log,log,3478,"0 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3769,deployability,fail,failed,3769,"IR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3823,deployability,fail,fail,3823,"_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3836,deployability,fail,failure,3836,".gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. Wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4143,deployability,fail,failed,4143,".fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4235,deployability,fail,failure,4235,"008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. Va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4825,deployability,log,log,4825," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1753,energy efficiency,model,models,1753,"you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2057,energy efficiency,model,models,2057,"(https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2091,energy efficiency,model,model,2091,"olumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2163,energy efficiency,model,model,2163,"ed with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2235,energy efficiency,model,model,2235,"` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2293,energy efficiency,model,model,2293,"ed from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3518,energy efficiency,core,core,3518,"C7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4782,energy efficiency,model,model,4782," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4869,energy efficiency,core,core,4869," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4926,energy efficiency,CPU,CPU,4926," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4956,energy efficiency,core,cores,4956," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5582,energy efficiency,Cloud,Cloud,5582," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5675,energy efficiency,Cloud,Cloud,5675," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3067,integrability,buffer,buffer,3067,"est data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all backgroun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:44,interoperability,format,format,44,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1946,modifiability,pac,packages,1946,"ake_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2004,performance,parallel,parallel,2004,"t, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2117,performance,disk,disk,2117,"ke sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3016,performance,time,time,3016," apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3048,performance,parallel,parallel,3048,"load models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without kil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3836,performance,failur,failure,3836,".gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. Wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4235,performance,failur,failure,4235,"008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. Va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4287,performance,error,errors,4287,"les.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the patte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4416,performance,error,error,4416,"{N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4630,performance,time,time,4630,"amples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4926,performance,CPU,CPU,4926," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5049,performance,error,errors,5049," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5204,performance,error,errors,5204," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3769,reliability,fail,failed,3769,"IR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3823,reliability,fail,fail,3823,"_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3836,reliability,fail,failure,3836,".gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. Wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4143,reliability,fail,failed,4143,".fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4235,reliability,fail,failure,4235,"008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. Va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4770,reliability,checkpoint,checkpoint,4770," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:224,safety,test,test,224,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:703,safety,test,test,703,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:785,safety,test,test,785,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1162,safety,test,tested,1162,"${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1903,safety,test,testdata,1903,"sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1972,safety,updat,update,1972,"posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your origin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2069,safety,test,test,2069,"cs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3478,safety,log,log,3478,"0 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4150,safety,compl,completely,4150,". --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4287,safety,error,errors,4287,"les.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the patte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4416,safety,error,error,4416,"{N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4825,safety,log,log,4825," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5049,safety,error,errors,5049," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5204,safety,error,errors,5204," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5385,safety,compl,completed,5385," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5482,safety,compl,complicated,5482," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1290,security,modif,modified,1290,"From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1753,security,model,models,1753,"you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1961,security,apt,apt-get,1961,"mmand you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1972,security,updat,update,1972,"posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your origin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1985,security,apt,apt-get,1985,"If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2019,security,apt,apt-get,2019,"make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2057,security,model,models,2057,"(https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2091,security,model,model,2091,"olumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2163,security,model,model,2163,"ed with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2235,security,model,model,2235,"` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2293,security,model,model,2293,"ed from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3478,security,log,log,3478,"0 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4150,security,compl,completely,4150,". --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4782,security,model,model,4782," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4825,security,log,log,4825," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5385,security,compl,completed,5385," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5482,security,compl,complicated,5482," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5681,security,team,team,5681," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5700,security,team,team,5700," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:224,testability,test,test,224,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:703,testability,test,test,703,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:785,testability,test,test,785,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1162,testability,test,tested,1162,"${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1903,testability,test,testdata,1903,"sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2069,testability,test,test,2069,"cs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3478,testability,log,log,3478,"0 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi. ```. Then, I ran `make_examples` similar to the way you did in your original post:. ```. ## Run `make_examples`. ( time seq 0 $((N_SHARDS-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""hs37d5.fa.gz"" \. --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4825,testability,log,log,4825," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:303,usability,command,command,303,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:768,usability,command,command,768,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:853,usability,command,command,853,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:963,usability,command,command,963,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated . `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1271,usability,command,commands,1271,"rom 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker. If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1375,usability,confirm,confirm,1375,"that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1456,usability,interact,interactive,1456,"umes/), but you still need to make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1486,usability,command,command,1486,"o make sure that the files are actually visible when/where you run call_variants. If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1651,usability,interact,interactive,1651,"les step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out. ```. sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2. ```. 2. Inside the interactive mode, run the following:. ```. MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard"". DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. N_SHARDS=""64"". ## Download extra packages. sudo apt-get -y update. sudo apt-get -y install parallel. sudo apt-get -y install aria2. ## Download models, and test data. # Copy the model files to your local disk. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index. aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_huma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4287,usability,error,errors,4287,"les.tfrecord@${N_SHARDS}.gz"" \. --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the patte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4356,usability,workflow,workflow,4356,"l_exon_v5_b37_targets.bed"" \. --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4416,usability,error,error,4416,"{N_SHARDS}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4536,usability,confirm,confirming,4536,". Before I proceeded with call_variants, I first checked that the output files from make_examples exist:. ```. ls HG002.examples.tfrecord*.gz | wc -l. ```. I see 64 of them here. A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4995,usability,confirm,confirm,4995," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5049,usability,error,errors,5049," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5175,usability,command,command,5175," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5204,usability,error,errors,5204," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5466,usability,workflow,workflow,5466," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:5727,usability,feedback,feedback,5727," but you didn't notice, then the next step will fail. Common failure modes I've seen before:. - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted. - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:. ```. ## Run `call_variants`. ( time \. /opt/deepvariant/bin/call_variants \. --outfile ""HG002.cvo.tfrecord.gz"" \. --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""model.ckpt"" \. ) 2>&1 | tee ""call_variants.log"" &. ```. When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:. ```. ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz"". ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,availability,failur,failure,138,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,deployability,fail,failure,138,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:304,energy efficiency,GPU,GPU,304,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,performance,failur,failure,138,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:304,performance,GPU,GPU,304,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:138,reliability,fail,failure,138,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:87,testability,understand,understand,87,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:76,usability,help,helpful,76,"@pichuan Thank for such a thorough description, really appreciate it. Super helpful to understand the different expectations and possible failure modes of each step. I will go back and make sure all of the preconditions you mentioned are met. One last comment though -- you didn't run call_variants on a GPU though it seems that is recommended; just makes it faster or is there any other reason to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:258,deployability,orchestr,orchestrate,258,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:333,deployability,instal,installing,333,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:483,deployability,instal,installing,483,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:892,deployability,build,build,892,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1002,deployability,depend,depending,1002,"n Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1248,deployability,configurat,configurations,1248,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:9,energy efficiency,Current,Currently,9,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:313,energy efficiency,GPU,GPU,313,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:344,energy efficiency,GPU,GPU,344,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:394,energy efficiency,GPU,GPU,394,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:427,energy efficiency,GPU,GPU,427,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:494,energy efficiency,GPU,GPU,494,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:743,energy efficiency,GPU,GPU,743,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:949,energy efficiency,GPU,GPUs,949,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1067,energy efficiency,Cloud,Cloud,1067,"sed to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1089,energy efficiency,cloud,cloud,1089," of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, pl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1351,energy efficiency,Cloud,Cloud,1351,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1373,energy efficiency,cloud,cloud,1373,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:258,integrability,orchestr,orchestrate,258,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1002,integrability,depend,depending,1002,"n Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1169,integrability,configur,configure,1169,"ine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1248,integrability,configur,configurations,1248,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:272,interoperability,distribut,distributed,272,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1002,modifiability,depend,depending,1002,"n Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1169,modifiability,configur,configure,1169,"ine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1248,modifiability,configur,configurations,1248,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1450,modifiability,maintain,maintains,1450,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:313,performance,GPU,GPU,313,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:344,performance,GPU,GPU,344,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:394,performance,GPU,GPU,394,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:427,performance,GPU,GPU,427,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:494,performance,GPU,GPU,494,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:743,performance,GPU,GPU,743,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:949,performance,GPU,GPUs,949,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1678,performance,time,time,1678,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1818,performance,perform,performance,1818,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1002,safety,depend,depending,1002,"n Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1450,safety,maintain,maintains,1450,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1169,security,configur,configure,1169,"ine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1248,security,configur,configurations,1248,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1445,security,team,team,1445,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1002,testability,depend,depending,1002,"n Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2164,testability,simpl,simplify,2164,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:284,usability,workflow,workflow,284,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:761,usability,document,documented,761,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1271,usability,workflow,workflow,1271,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1298,usability,learn,learn,1298,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1784,usability,person,personally,1784,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1818,usability,perform,performance,1818,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2054,usability,document,documentation,2054,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2164,usability,simpl,simplify,2164,"cus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc). If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU. We have also documented it here:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run. If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --. Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/pull/152:345,deployability,patch,patch,345,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:336,integrability,sub,submit,336,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:345,safety,patch,patch,345,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:366,safety,test,testing,366,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:345,security,patch,patch,345,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:366,testability,test,testing,366,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:53,deployability,releas,releasing,53,I am fine with you making this change internally and releasing in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:75,deployability,releas,release,75,I am fine with you making this change internally and releasing in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:467,availability,sli,slightly,467,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:45,deployability,updat,update,45,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:92,deployability,releas,release,92,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:182,deployability,releas,release,182,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:416,deployability,configurat,configuration,416,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:878,deployability,releas,release,878,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:416,integrability,configur,configuration,416,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:680,integrability,configur,configured,680,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:416,modifiability,configur,configuration,416,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:680,modifiability,configur,configured,680,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:467,reliability,sli,slightly,467,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:45,safety,updat,update,45,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:45,security,updat,update,45,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:416,security,configur,configuration,416,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:680,security,configur,configured,680,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:864,security,auth,author,864,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:486,usability,tool,tool,486,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:346,availability,consist,consistent,346,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:15,deployability,updat,updates,15,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:96,deployability,updat,update,96,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:157,energy efficiency,current,current,157,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:298,integrability,event,eventually,298,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:15,safety,updat,updates,15,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:96,safety,updat,update,96,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:130,safety,test,testing,130,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:15,security,updat,updates,15,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:96,security,updat,update,96,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:130,testability,test,testing,130,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:322,usability,confirm,confirm,322,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:346,usability,consist,consistent,346,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:42,deployability,releas,release,42,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0. Thank you for your contribution! @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:89,deployability,releas,releases,89,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0. Thank you for your contribution! @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:164,energy efficiency,schedul,schedule,164,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0. Thank you for your contribution! @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:164,performance,schedul,schedule,164,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0. Thank you for your contribution! @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:24,deployability,updat,update,24,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:166,energy efficiency,current,current,166,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:481,energy efficiency,current,currently,481,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:636,energy efficiency,current,currently,636,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:705,energy efficiency,current,current,705,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:230,reliability,doe,doesn,230,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:24,safety,updat,update,24,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:302,safety,compl,complete,302,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:858,safety,compl,completely,858,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:24,security,updat,update,24,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:75,security,team,team,75,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:302,security,compl,complete,302,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:725,security,team,team,725,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:858,security,compl,completely,858,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:713,testability,plan,plan,713,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:142,usability,Close,Close,142,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:787,usability,confirm,confirm,787,"Hi all,. I'm posting an update to this PR after talking to @fo40225: . Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:. 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off). 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/issues/153:92,modifiability,interm,intermediate,92,"Hi @ekofman, yes that's a reasonable output size. For reference, our 110 GB BAM produces an intermediate output of 42 GB, during the make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/154:32,availability,error,error,32,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:77,availability,error,errors,77,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:162,deployability,version,version,162,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:182,deployability,releas,release,182,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:233,deployability,API,API,233,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:381,deployability,instal,install,381,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:38,integrability,messag,messages,38,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:162,integrability,version,version,162,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:233,integrability,API,API,233,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:38,interoperability,messag,messages,38,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:233,interoperability,API,API,233,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:162,modifiability,version,version,162,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:32,performance,error,error,32,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:77,performance,error,errors,77,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:32,safety,error,error,32,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:72,safety,test,test,72,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:77,safety,error,errors,77,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:439,safety,test,tests,439,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:72,testability,test,test,72,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:439,testability,test,tests,439,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:32,usability,error,error,32,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:77,usability,error,errors,77,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:96,usability,user,users,96,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:391,usability,user,user,391,"Hi @qili93 . can you paste your error messages here? Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:. https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86. to:. ```. pip install --user 'intervaltree==2.1.0'. ```. and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:203,availability,error,error,203,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:60,deployability,fail,failed,60,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:170,deployability,fail,failed,170,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:235,deployability,FAIL,FAIL,235,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:209,integrability,messag,message,209,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:209,interoperability,messag,message,209,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1808,modifiability,paramet,parameterized,1808," ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2278,modifiability,pac,packages,2278,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2432,modifiability,pac,packages,2432,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:203,performance,error,error,203,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1095,performance,cach,cache,1095,"mples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1362,performance,cach,cache,1362,"ases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1624,performance,cach,cache,1624,"0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1909,performance,cach,cache,1909,"xpected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". star",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2564,performance,cach,cache,2564,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:60,reliability,fail,failed,60,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:170,reliability,fail,failed,170,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:235,reliability,FAIL,FAIL,235,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:203,safety,error,error,203,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1324,safety,test,testPartExecutor,1324,"reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_onc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1597,safety,test,testMethod,1597,"xpected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1800,safety,test,testing,1800," start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1046,testability,Trace,Traceback,1046," another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1324,testability,test,testPartExecutor,1324,"reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_onc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1597,testability,test,testMethod,1597,"xpected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1800,testability,test,testing,1800," start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2287,testability,mock,mock,2287,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2292,testability,mock,mock,2292,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2441,testability,mock,mock,2441,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2446,testability,mock,mock,2446,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2504,testability,Assert,AssertionError,2504,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2786,testability,Assert,AssertionError,2786,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2957,testability,mock,mock,2957,"----------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref. ranges.make_range('20', expected_start, expected_end)). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with. return self.assert_called_with(*args, **kwargs). File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with. six.raise_from(AssertionError(_error_message(cause)), cause). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from. raise value. AssertionError: Expected call: query(reference_name: ""20"". start: 9. end: 21. ). Actual call: query(reference_name: ""20"". start: 9. end: 1 <== this ends with 1 due to the mock object returns 1. ). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:203,usability,error,error,203,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```. FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". alternate_bases: ""C"". end: 11. reference_name: ""20"". start: 10. , reference_bases: ""A"". alternate_bases: ""C"". end: 21. reference_name: ""20"". start: 20. ]) (__main__.HaplotypeLabelerClassUnitTest). test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A"". ----------------------------------------------------------------------. Traceback (most recent call last):. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor. yield. File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run. testMethod(). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test. test_method(self, **testcase_params). File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:45,availability,error,error,45,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:173,deployability,build,build-prereq,173,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:196,deployability,instal,install,196,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:421,deployability,patch,patch,421,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:669,deployability,build,build,669,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:725,deployability,contain,container,725,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:789,deployability,build,build,789,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:327,energy efficiency,Current,Currently,327,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:524,energy efficiency,current,currently,524,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:390,interoperability,platform,platform,390,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:45,performance,error,error,45,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:45,safety,error,error,45,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:240,safety,test,test,240,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:421,safety,patch,patch,421,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:421,security,patch,patch,421,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:240,testability,test,test,240,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:628,testability,understand,understand,628,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:45,usability,error,error,45,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:206,usability,user,user,206,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:380,usability,support,supported,380,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:620,usability,help,help,620,"Hi @qili93 . I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine). ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'. bazel test -c opt //deepvariant/labeler:haplotype_labeler_test. ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:123,energy efficiency,cool,cool,123,"Wait @pichuan, are you saying you're not even going to have a DeepVariant app at the Google Play store? That would be sooo cool ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1255,availability,error,error,1255,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1374,deployability,modul,module,1374,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1542,deployability,upgrad,upgrade,1542,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:680,energy efficiency,Power,Power,680,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1495,interoperability,convers,conversion,1495,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1374,modifiability,modul,module,1374,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1542,modifiability,upgrad,upgrade,1542,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1255,performance,error,error,1255,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1179,safety,accid,accidentally,1179,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1255,safety,error,error,1255,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1374,safety,modul,module,1374,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:242,testability,mock,mock,242,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:436,testability,mock,mock,436,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:721,testability,mock,mock,721,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:915,testability,mock,mock,915,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1105,testability,mock,mock,1105,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1310,testability,Trace,Traceback,1310,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1518,testability,plan,plan,1518,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:160,usability,command,commands,160,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1206,usability,command,command,1206,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1255,usability,error,error,1255,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1402,usability,support,supported,1402,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1526,usability,support,support,1526,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. 21. ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```. >>> import mock. >>>. >>> expected_start=9. >>> expected_end=21. >>> bufsize=0. >>> expected_bases = 'A' * (expected_end - expected_start). >>>. >>> start=10. >>> end=21. >>> contig='20'. >>> ref_reader = mock.MagicMock(). >>> ref_reader.query.return_value = expected_bases. >>> contig_nbp = ref_reader.contig(contig).n_bases. >>> res = min(end + bufsize, contig_nbp). >>> res. <MagicMock name='mock.contig().n_bases' id='140373647899088'>. >>> int(res). 1. ```. And I accidentally run the above command with Python3, and it throw the following error:. ```. >>> res = min(end + bufsize, contig_nbp). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. TypeError: '<' not supported between instances of 'MagicMock' and 'int'. ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:90,deployability,upgrad,upgrade,90,"Hi @qili93 . Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. . Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:90,modifiability,upgrad,upgrade,90,"Hi @qili93 . Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. . Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:114,performance,time,timeline,114,"Hi @qili93 . Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. . Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:183,testability,plan,plans,183,"Hi @qili93 . Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. . Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:177,usability,clear,clear,177,"Hi @qili93 . Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. . Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:173,deployability,releas,release,173,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:230,deployability,version,version,230,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:304,deployability,BUILD,BUILD,304,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:362,deployability,depend,dependency,362,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:230,integrability,version,version,230,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:362,integrability,depend,dependency,362,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:230,modifiability,version,version,230,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:362,modifiability,depend,dependency,362,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:53,reliability,doe,does,53,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:362,safety,depend,dependency,362,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:214,security,modif,modify,214,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:37,testability,mock,mock,37,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:362,testability,depend,dependency,362,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:508,testability,mock,mock,508,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:. ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42. from third_party.nucleus.protos import reference_pb2. ### Insert at line 346. labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/155:244,deployability,instal,install,244,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:320,deployability,upgrad,upgraded,320,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:380,deployability,version,version,380,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:422,deployability,releas,release,422,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:380,integrability,version,version,380,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:320,modifiability,upgrad,upgraded,320,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:380,modifiability,version,version,380,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:341,performance,time,time,341,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:395,testability,plan,plan,395,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:254,usability,user,user,254,"Hi @internalsensor ,. can you tell us how you're running DeepVariant? Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image? If it's the first or second, you need to change run-prereq.sh to. ```. pip install --user 'intervaltree==2.1.0'. ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:2,availability,down,downloaded,2,"I downloaded and ran the pre-built binaries *.zip (DeepVariant v 7.0.2) from GitHub. In fact, I have installed 'intervaltree==2.1.0', but it does not works? Strange.....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:101,deployability,instal,installed,101,"I downloaded and ran the pre-built binaries *.zip (DeepVariant v 7.0.2) from GitHub. In fact, I have installed 'intervaltree==2.1.0', but it does not works? Strange.....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:141,reliability,doe,does,141,"I downloaded and ran the pre-built binaries *.zip (DeepVariant v 7.0.2) from GitHub. In fact, I have installed 'intervaltree==2.1.0', but it does not works? Strange.....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:367,availability,down,download,367,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:48,deployability,version,versions,48,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:57,deployability,instal,installed,57,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:195,deployability,modul,module,195,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:202,deployability,instal,installed,202,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:486,deployability,modul,modules,486,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:48,integrability,version,versions,48,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:48,modifiability,version,versions,48,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:195,modifiability,modul,module,195,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:380,modifiability,pac,package,380,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:486,modifiability,modul,modules,486,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:555,modifiability,pac,packages,555,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:538,reliability,doe,does-python-find-packages,538,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:195,safety,modul,module,195,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:486,safety,modul,modules,486,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:100,usability,command,commands,100,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:578,usability,help,helps,578,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python. import sys. print(sys.path). ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:376,availability,error,error,376,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:574,availability,down,downloaded,574,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1018,availability,error,error,1018,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:117,deployability,version,version,117,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:270,deployability,version,versions,270,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:279,deployability,instal,installed,279,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:472,deployability,version,version,472,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:526,deployability,instal,install,526,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:543,deployability,version,version,543,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:117,integrability,version,version,117,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:270,integrability,version,versions,270,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:472,integrability,version,version,472,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:543,integrability,version,version,543,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:117,modifiability,version,version,117,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:270,modifiability,version,versions,270,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:472,modifiability,version,version,472,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:543,modifiability,version,version,543,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1091,modifiability,pac,package,1091,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:376,performance,error,error,376,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1018,performance,error,error,1018,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:376,safety,error,error,376,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1018,safety,error,error,1018,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:463,security,modif,modified,463,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:76,usability,feedback,feedback,76,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:376,usability,error,error,376,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1018,usability,error,error,1018,"Hi @internalsensor , please see my answer below. (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===. Hi @internalsensor , . I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found. In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh. bash -x run_wes_case_study_prebuilt_binaries.sh. ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error. You can also use `pip show intervaltree` to double check what pip package you have. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:113,deployability,version,version,113,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:172,deployability,version,version,172,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:113,integrability,version,version,113,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:172,integrability,version,version,172,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:113,modifiability,version,version,113,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:172,modifiability,version,version,172,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:35,usability,close,close,35,"Hi @internalsensor,. I am going to close this issue. If you have more questions, please let us know. In the next version, we'll make sure the pin intervaltree to the right version that goes with the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/157:7,deployability,depend,depends,7,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:7,integrability,depend,depends,7,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:7,modifiability,depend,depends,7,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:7,safety,depend,depends,7,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:7,testability,depend,depends,7,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:100,energy efficiency,Core,Cores,100,Lets say I want to do an analysis on a 9gb WES bam file. Will 32gb be enough? What about the Tensor Cores of my GPU?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:112,energy efficiency,GPU,GPU,112,Lets say I want to do an analysis on a 9gb WES bam file. Will 32gb be enough? What about the Tensor Cores of my GPU?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:112,performance,GPU,GPU,112,Lets say I want to do an analysis on a 9gb WES bam file. Will 32gb be enough? What about the Tensor Cores of my GPU?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,deployability,depend,depends,116,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,integrability,depend,depends,116,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,modifiability,depend,depends,116,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,safety,depend,depends,116,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,testability,depend,depends,116,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:170,availability,consist,consistency,170,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:78,deployability,configurat,configuration,78,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:70,energy efficiency,current,current,70,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:251,energy efficiency,current,current,251,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:78,integrability,configur,configuration,78,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:78,modifiability,configur,configuration,78,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:17,performance,perform,performing,17,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:134,performance,memor,memory,134,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:78,security,configur,configuration,78,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:17,usability,perform,performing,17,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:116,usability,minim,minimum,116,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:134,usability,memor,memory,134,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:170,usability,consist,consistency,170,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:229,usability,satisfa,satisfactory,229,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:50,availability,slo,slots,50,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:76,deployability,upgrad,upgrade,76,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:0,energy efficiency,Current,Currently,0,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:191,energy efficiency,core,cores,191,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:259,energy efficiency,cpu,cpus,259,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:76,modifiability,upgrad,upgrade,76,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:259,performance,cpu,cpus,259,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:50,reliability,slo,slots,50,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:904,availability,state,statement,904,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:110,deployability,releas,release,110,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:393,deployability,configurat,configuration,393,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:606,deployability,configurat,configuration,606,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:621,deployability,continu,continue,621,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:684,deployability,build,building,684,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:762,deployability,Continu,Continue,762,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:863,deployability,observ,observations,863,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1386,deployability,version,version,1386,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:128,energy efficiency,current,currently,128,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:598,energy efficiency,current,current,598,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:702,energy efficiency,optim,optimized,702,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1082,energy efficiency,Core,Cores,1082,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1210,energy efficiency,CPU,CPU,1210,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1214,energy efficiency,GPU,GPU,1214,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1417,energy efficiency,Core,Cores,1417,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:393,integrability,configur,configuration,393,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:606,integrability,configur,configuration,606,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:904,integrability,state,statement,904,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1177,integrability,interfac,interface,1177,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1386,integrability,version,version,1386,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:269,interoperability,share,share,269,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:384,interoperability,specif,specific,384,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1177,interoperability,interfac,interface,1177,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:393,modifiability,configur,configuration,393,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:606,modifiability,configur,configuration,606,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1177,modifiability,interfac,interface,1177,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1386,modifiability,version,version,1386,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:181,performance,perform,performance,181,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:702,performance,optimiz,optimized,702,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1210,performance,CPU,CPU,1210,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1214,performance,GPU,GPU,1214,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:331,security,team,team,331,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:393,security,configur,configuration,393,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:606,security,configur,configuration,606,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:863,testability,observ,observations,863,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:181,usability,perform,performance,181,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:219,usability,learn,learn,219,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:568,usability,document,documenting,568,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:774,usability,document,document,774,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:936,usability,document,documentations,936,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1277,usability,user,users,1277,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:1540,usability,close,close,1540,"Hi @kokyriakidis . MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release. We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/158:752,availability,error,error,752,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:752,performance,error,error,752,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:752,safety,error,error,752,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:62,testability,understand,understand,62,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:538,testability,understand,understand,538,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:51,usability,help,help,51,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:533,usability,help,help,533,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:728,usability,command,commands,728,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:752,usability,error,error,752,"Hello Ferdinand,. I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:35,deployability,version,version,35,Probably it would be better to use version 0.7.2 of DV as there was a bugfix for gVCF creation in 0.5.2 from 0.5.0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:35,integrability,version,version,35,Probably it would be better to use version 0.7.2 of DV as there was a bugfix for gVCF creation in 0.5.2 from 0.5.0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:35,modifiability,version,version,35,Probably it would be better to use version 0.7.2 of DV as there was a bugfix for gVCF creation in 0.5.2 from 0.5.0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:772,availability,error,error,772,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1164,integrability,messag,message,1164,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1164,interoperability,messag,message,1164,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:772,performance,error,error,772,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:772,safety,error,error,772,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:70,testability,understand,understand,70,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:558,testability,understand,understand,558,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:59,usability,help,help,59,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:553,usability,help,help,553,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:748,usability,command,commands,748,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:772,usability,error,error,772,"> Hello Ferdinand,. > . > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l). > . > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers. > . > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error. > . > Thanks,. > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data. As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:559,energy efficiency,current,current,559,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:367,performance,parallel,parallel,367,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:409,performance,parallel,parallel,409,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:747,performance,parallel,parallel,747,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:74,usability,command,command,74,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:173,usability,command,command,173,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:227,usability,indicat,indicates,227,"Hi Ferdinand,. Thank you for the region files. I was looking through your command, and I think I realize the source of the fewer number of variants. . In your make examples command, you have --task 0. Your make examples script indicates that there will be 8 shards, but this will only run the first of those shards. You will need to run all 8 tasks (and can do so in parallel). You can see the way we use gnu parallel in the exome case study script (https://github.com/google/deepvariant/blob/r0.7/scripts/run_wes_case_study_docker.sh). As a result, with the current way you execute the script, this will only call variants in 1/8 of the examples. . Please let me know if you have questions about this, or about how to run with the tasks with gnu parallel. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/pull/159:4,security,sign,signed,4,> I signed it!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:410,deployability,log,log,410,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:219,safety,test,test,219,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:410,safety,log,log,410,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:410,security,log,log,410,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:219,testability,test,test,219,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:410,testability,log,log,410,"Hi @A-Tsai ,. I think our setup on GitHub might be a bit confusing right now. Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. . We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. Please let us know if you're ok with that. If so, we'll proceed with that. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:420,deployability,log,log,420,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:229,safety,test,test,229,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:420,safety,log,log,420,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:420,security,log,log,420,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:229,testability,test,test,229,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:420,testability,log,log,420,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:559,testability,verif,verify,559,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:551,usability,help,help,551,"> Hi @A-Tsai ,. > I think our setup on GitHub might be a bit confusing right now. > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests. > . > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log. > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:142,availability,avail,available,142,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:226,deployability,Updat,Update,226,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:65,energy efficiency,GPU,GPU,65,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:131,energy efficiency,GPU,GPU,131,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:169,energy efficiency,CPU,CPU,169,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:203,energy efficiency,core,core,203,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:395,energy efficiency,model,model,395,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:435,energy efficiency,estimat,estimator,435,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:89,interoperability,specif,specified,89,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:280,interoperability,specif,specified,280,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:65,performance,GPU,GPU,65,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:111,performance,memor,memory,111,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:131,performance,GPU,GPU,131,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:169,performance,CPU,CPU,169,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:142,reliability,availab,available,142,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:142,safety,avail,available,142,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:226,safety,Updat,Update,226,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:336,safety,sanit,sanity,336,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:142,security,availab,available,142,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:226,security,Updat,Update,226,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:336,security,sanit,sanity,336,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:395,security,model,model,395,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:30,testability,understand,understand,30,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:111,usability,memor,memory,111,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct? Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:208,deployability,resourc,resource,208,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:311,deployability,configurat,configuration,311,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:188,energy efficiency,CPU,CPU,188,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:204,energy efficiency,CPU,CPU,204,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:208,energy efficiency,resourc,resource,208,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:217,energy efficiency,alloc,allocation,217,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:328,energy efficiency,CPU,CPU,328,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:311,integrability,configur,configuration,311,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:311,modifiability,configur,configuration,311,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:188,performance,CPU,CPU,188,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:204,performance,CPU,CPU,204,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:208,performance,resourc,resource,208,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:328,performance,CPU,CPU,328,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:370,performance,overhead,overhead,370,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:467,performance,time,time,467,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:292,reliability,doe,doesn,292,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:208,safety,resourc,resource,208,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:266,security,threat,threat,266,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:311,security,configur,configuration,311,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:208,testability,resourc,resource,208,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:386,testability,context,context,386,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:112,usability,minim,minimize,112,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:303,availability,sli,slightly,303,"@A-Tsai In the next release, we plan to add a flag to `call_variants` that will allow users to pass in any desired configuration options for the TensorFlow session config. We won't hard code any options, but you will be able to pass in all of the options included in this pull request. The code will be slightly different from what you have specified as the session configuration will be passed to [the estimator](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L344), rather than [this code block](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L330). Thanks for the suggested changes!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
